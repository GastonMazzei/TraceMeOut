{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraceMeOut-4GoogleColab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Required Imports</b>"
      ],
      "metadata": {
        "id": "S6WiCYxRYJMy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NJdGVNSkPlx",
        "outputId": "e991aff7-81a9-47a1-eb79-987ba129d892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "import pickle,sys,os, time\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset0.pkl?raw=true\n",
        "!wget https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset1.pkl?raw=true\n",
        "!wget https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset2.pkl?raw=true\n",
        "!wget https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset3.pkl?raw=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRjDxDxSlffM",
        "outputId": "0c2e8982-712f-45ef-828e-40c3b41af837"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-31 00:35:04--  https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset0.pkl?raw=true\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset0.pkl [following]\n",
            "--2021-12-31 00:35:05--  https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset0.pkl\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset0.pkl [following]\n",
            "--2021-12-31 00:35:05--  https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset0.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18299973 (17M) [application/octet-stream]\n",
            "Saving to: ‘Dataset0.pkl?raw=true.6’\n",
            "\n",
            "Dataset0.pkl?raw=tr 100%[===================>]  17.45M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-12-31 00:35:05 (236 MB/s) - ‘Dataset0.pkl?raw=true.6’ saved [18299973/18299973]\n",
            "\n",
            "--2021-12-31 00:35:05--  https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset1.pkl?raw=true\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset1.pkl [following]\n",
            "--2021-12-31 00:35:05--  https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset1.pkl\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset1.pkl [following]\n",
            "--2021-12-31 00:35:05--  https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset1.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24479477 (23M) [application/octet-stream]\n",
            "Saving to: ‘Dataset1.pkl?raw=true.6’\n",
            "\n",
            "Dataset1.pkl?raw=tr 100%[===================>]  23.34M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2021-12-31 00:35:05 (277 MB/s) - ‘Dataset1.pkl?raw=true.6’ saved [24479477/24479477]\n",
            "\n",
            "--2021-12-31 00:35:06--  https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset2.pkl?raw=true\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset2.pkl [following]\n",
            "--2021-12-31 00:35:06--  https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset2.pkl\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset2.pkl [following]\n",
            "--2021-12-31 00:35:06--  https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset2.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12815767 (12M) [application/octet-stream]\n",
            "Saving to: ‘Dataset2.pkl?raw=true.6’\n",
            "\n",
            "Dataset2.pkl?raw=tr 100%[===================>]  12.22M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-12-31 00:35:06 (271 MB/s) - ‘Dataset2.pkl?raw=true.6’ saved [12815767/12815767]\n",
            "\n",
            "--2021-12-31 00:35:06--  https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset3.pkl?raw=true\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset3.pkl [following]\n",
            "--2021-12-31 00:35:06--  https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset3.pkl\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset3.pkl [following]\n",
            "--2021-12-31 00:35:06--  https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset3.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42367432 (40M) [application/octet-stream]\n",
            "Saving to: ‘Dataset3.pkl?raw=true.6’\n",
            "\n",
            "Dataset3.pkl?raw=tr 100%[===================>]  40.40M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-12-31 00:35:07 (298 MB/s) - ‘Dataset3.pkl?raw=true.6’ saved [42367432/42367432]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeBPa_RrlnaD",
        "outputId": "ce60c8c9-9e67-4f78-f544-4f3ab44636fe"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Dataset0.pkl?raw=true'    'Dataset1.pkl?raw=true.3'  'Dataset2.pkl?raw=true.6'\n",
            "'Dataset0.pkl?raw=true.1'  'Dataset1.pkl?raw=true.4'  'Dataset3.pkl?raw=true'\n",
            "'Dataset0.pkl?raw=true.2'  'Dataset1.pkl?raw=true.5'  'Dataset3.pkl?raw=true.1'\n",
            "'Dataset0.pkl?raw=true.3'  'Dataset1.pkl?raw=true.6'  'Dataset3.pkl?raw=true.2'\n",
            "'Dataset0.pkl?raw=true.4'  'Dataset2.pkl?raw=true'    'Dataset3.pkl?raw=true.3'\n",
            "'Dataset0.pkl?raw=true.5'  'Dataset2.pkl?raw=true.1'  'Dataset3.pkl?raw=true.4'\n",
            "'Dataset0.pkl?raw=true.6'  'Dataset2.pkl?raw=true.2'  'Dataset3.pkl?raw=true.5'\n",
            "'Dataset1.pkl?raw=true'    'Dataset2.pkl?raw=true.3'  'Dataset3.pkl?raw=true.6'\n",
            "'Dataset1.pkl?raw=true.1'  'Dataset2.pkl?raw=true.4'   sample_data\n",
            "'Dataset1.pkl?raw=true.2'  'Dataset2.pkl?raw=true.5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datas = {}\n",
        "for n in [0,1,2,3]:\n",
        "  with open(f'Dataset{n}.pkl?raw=true','rb') as f:\n",
        "    datas[n] = pickle.load(f)"
      ],
      "metadata": {
        "id": "_AFenX9QlrWx"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<b>Configuration File: architecture & hyperparameters</b>\n",
        "\n",
        "(<i>If the datasets change in the repo, non-architectural information should be updated using the configuration.py file. Most probably an error will be raised because of size mismatch if this is not updated.</i>)"
      ],
      "metadata": {
        "id": "ZoqNt7-YYn-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model-specific parameters\n",
        "T=8 # duration of the window in dt units\n",
        "dt = 4000 # time in microseconds\n",
        "UNIQUES=3807  #number of unique ids\n",
        "MI=4762  #max number of interactions\n",
        "ML=4765  #max number of leaves\n",
        "NCATEGORIES=2\n",
        "\n",
        "# Architectural parameters\n",
        "ACT1 = 'relu'\n",
        "FILTERS1 = 8\n",
        "KSIZE1 = (2,1)\n",
        "PSIZE1 = (max([T//4,2]),)\n",
        "NDENSE1 = 8\n",
        "DROP1 = 0.4\n",
        "\n",
        "ACT2 = 'relu'\n",
        "FILTERS2 = 8\n",
        "KSIZE2 = (2,2)\n",
        "PSIZE2 = (max([T//2,2]),1)\n",
        "stride = (1,1)\n",
        "NDENSE2 = 8\n",
        "DROP2 = 0.5\n",
        "\n",
        "\n",
        "ACT4='relu'\n",
        "NDENSE4=8\n",
        "DROP4 = 0.3\n",
        "\n",
        "\n",
        "ACT3='relu'\n",
        "NDENSE3=8\n",
        "DROP3 = 0.3\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "VAL=0.25\n",
        "BATCH=128\n",
        "EPOCHS=50\n",
        "L=5 # a length used to generate random data just for testing\n",
        "LR=0.05\n",
        "SAMPLES = 30\n",
        "\n",
        "\n",
        "# Extras\n",
        "POOLING = False\n",
        "PROCS=[3, 2, 0, 1]"
      ],
      "metadata": {
        "id": "nsC-H7jvkmMn"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assert the dataset's keys and the procs in the configuration are the same :-)\n",
        "temp1, temp2 = sorted(PROCS), sorted(list(datas.keys()))\n",
        "assert(temp1 == temp2)\n",
        "NPROCS = len(PROCS)\n",
        "\n",
        "# define two sets of inputs: \n",
        "MAXPAD = max([MI,ML])\n",
        "input_shape_flavours = (BATCH, T, MAXPAD, 1)\n",
        "input_shape_structure = (BATCH, T, MAXPAD, 2)\n",
        "\n",
        "def produce_model():\n",
        "\tinputFlavours = tf.keras.Input(shape=input_shape_flavours[1:])\n",
        "\tinputStructure = tf.keras.Input(shape=input_shape_structure[1:])\n",
        "\n",
        "\t# the first branch operates on the first input (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)\n",
        "\tx = tf.keras.layers.Conv2D(\n",
        "\t\t\t\t# Filters, Kersize, Strides, Padding,  Activation\n",
        "\t\t\t\tFILTERS1,         KSIZE1,       (1,1),      'valid',  activation = ACT1,\n",
        "\t\t\t\tinput_shape = input_shape_flavours[1:]\n",
        "\t\t\t\t)(inputFlavours)\n",
        "\tx = tf.keras.layers.Conv2D(\n",
        "\t\t\t\t# Filters, Kersize, Strides, Padding,  Activation\n",
        "\t\t\t\tFILTERS1 * 2,         KSIZE1,       (1,1),      'valid',  activation = ACT1,\n",
        "\t\t\t\t)(x)\n",
        "\tx = tf.keras.layers.Conv2D(\n",
        "\t\t\t\t# Filters, Kersize, Strides, Padding,  Activation\n",
        "\t\t\t\tFILTERS1 * 2,         KSIZE1,       (1,1),      'valid',  activation = ACT1,\n",
        "\t\t\t\t)(x)\n",
        "\t#x = tf.keras.layers.MaxPool1D(pool_size=PSIZE1)(x)\n",
        "\tx = tf.keras.layers.Flatten()(x)\n",
        "\tx = tf.keras.layers.Dropout(DROP1)(x)\n",
        "\tx = tf.keras.layers.Dense(NDENSE1, activation = ACT1)(x)\n",
        "\tx = tf.keras.layers.Dropout(DROP1)(x)\n",
        "\tx = tf.keras.layers.Dense(NDENSE1 // 2, activation = ACT1)(x)\n",
        "\tx = tf.keras.layers.Dropout(DROP1)(x)\n",
        "\tx = tf.keras.layers.Dense(NDENSE1 // 2 // 2, activation = ACT1)(x)\n",
        "\tx = tf.keras.Model(inputs = inputFlavours, outputs=x)\n",
        "\n",
        "\t# the second branch opreates on the second input (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)\n",
        "\ty = tf.keras.layers.Conv2D(\n",
        "\t\t\t\t# Filters, Kersize, Strides, Padding,  Activation\n",
        "\t\t\t\tFILTERS2,         KSIZE2,    stride,      'valid',  activation = ACT2,\n",
        "\t\t\t\tinput_shape = input_shape_structure[1:]\n",
        "\t\t\t\t)(inputStructure)\n",
        "\ty = tf.keras.layers.Conv2D(\n",
        "\t\t\t\t# Filters, Kersize, Strides, Padding,  Activation\n",
        "\t\t\t\tFILTERS2 * 2,         KSIZE2,       stride,      'valid',  activation = ACT2,\n",
        "\t\t\t\t)(y)\n",
        "\t#y = tf.keras.layers.MaxPool2D(pool_size=PSIZE2)(y)\n",
        "\ty = tf.keras.layers.Conv2D(\n",
        "\t\t\t\t# Filters, Kersize, Strides, Padding,  Activation\n",
        "\t\t\t\tFILTERS2 * 2,         KSIZE2,       stride,      'valid',  activation = ACT2,\n",
        "\t\t\t\t)(y)\n",
        "\ty = tf.keras.layers.MaxPool2D(pool_size=PSIZE2)(y)\n",
        "\ty = tf.keras.layers.Flatten()(y)\n",
        "\ty = tf.keras.layers.Dropout(DROP2)(y)\n",
        "\ty = tf.keras.layers.Dense(NDENSE2, activation = ACT2)(y)\n",
        "\ty = tf.keras.layers.Dropout(DROP2)(y)\n",
        "\ty = tf.keras.layers.Dense(NDENSE2 // 2, activation = ACT2)(y)\n",
        "\ty = tf.keras.layers.Dropout(DROP2)(y)\n",
        "\ty = tf.keras.layers.Dense(NDENSE2 // 2 // 2, activation = ACT2)(y)\n",
        "\ty = tf.keras.Model(inputs = inputStructure, outputs=y)\n",
        "\n",
        "\n",
        "\n",
        "\t# combine the output of the two branches\n",
        "\tcombined = tf.keras.layers.concatenate([x.output, y.output])\n",
        "\tz = tf.keras.layers.Dense(NDENSE3, activation = ACT3)(combined)\n",
        "\tz = tf.keras.layers.Dropout(DROP3)(z)\n",
        "\tz = tf.keras.layers.Dense(NDENSE3, activation = ACT3)(z)\n",
        "\tz = tf.keras.layers.Dropout(DROP3)(z)\n",
        "\tz = tf.keras.layers.Dense(NDENSE3, activation = ACT3)(z)\n",
        "\n",
        "\t# our model will accept the inputs of the two branches and\n",
        "\t# then output a single value\n",
        "\treturn tf.keras.Model(inputs=[x.input, y.input], outputs=z)\n",
        "\n",
        "models = []\n",
        "for n in range(NPROCS):\n",
        "\tmodels += [produce_model()]\n",
        "TOTALINPUTS = []\n",
        "for m in models:\n",
        "\tTOTALINPUTS += [m.inputs[0], m.inputs[1]]\n",
        "combined = tf.keras.layers.concatenate([m.output for m in models])\n",
        "w = tf.keras.layers.Dense(NDENSE4, activation = ACT4)(combined)\n",
        "w = tf.keras.layers.Dropout(DROP4)(w)\n",
        "w = tf.keras.layers.Dense(NDENSE4, activation = ACT4)(combined)\n",
        "w = tf.keras.layers.Dropout(DROP4)(w)\n",
        "w = tf.keras.layers.Dense(NDENSE4, activation = ACT4)(combined)\n",
        "w = tf.keras.layers.Dense(NCATEGORIES, activation=\"softmax\")(w)\n",
        "model = tf.keras.Model(inputs=TOTALINPUTS, outputs=w)\n",
        "\n",
        "# Compile the model :-)\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=LR),\n",
        "\t\t\t\t\t\t\tloss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "\t\t\t\t\t\t\tmetrics=[\n",
        "\t\t\t\t\t\t\t\t\t\t\t#tf.keras.metrics.CategoricalCrossentropy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\ttf.keras.metrics.CategoricalAccuracy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t#tf.keras.metrics.AUC(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t])\n",
        "\n",
        "# Print input size\n",
        "print(model.summary())\n",
        "IShape = model.input_shape\n",
        "OShape = model.output_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4gcQStkS5q",
        "outputId": "30266ce6-d4a7-4620-fa38-6afa93edb706"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_38\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_18 (InputLayer)          [(None, 8, 4765, 2)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_20 (InputLayer)          [(None, 8, 4765, 2)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_22 (InputLayer)          [(None, 8, 4765, 2)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_24 (InputLayer)          [(None, 8, 4765, 2)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_17 (InputLayer)          [(None, 8, 4765, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 7, 4764, 8)   72          ['input_18[0][0]']               \n",
            "                                                                                                  \n",
            " input_19 (InputLayer)          [(None, 8, 4765, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " conv2d_57 (Conv2D)             (None, 7, 4764, 8)   72          ['input_20[0][0]']               \n",
            "                                                                                                  \n",
            " input_21 (InputLayer)          [(None, 8, 4765, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " conv2d_63 (Conv2D)             (None, 7, 4764, 8)   72          ['input_22[0][0]']               \n",
            "                                                                                                  \n",
            " input_23 (InputLayer)          [(None, 8, 4765, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " conv2d_69 (Conv2D)             (None, 7, 4764, 8)   72          ['input_24[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 7, 4765, 8)   24          ['input_17[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 6, 4763, 16)  528         ['conv2d_51[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_54 (Conv2D)             (None, 7, 4765, 8)   24          ['input_19[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_58 (Conv2D)             (None, 6, 4763, 16)  528         ['conv2d_57[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_60 (Conv2D)             (None, 7, 4765, 8)   24          ['input_21[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_64 (Conv2D)             (None, 6, 4763, 16)  528         ['conv2d_63[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_66 (Conv2D)             (None, 7, 4765, 8)   24          ['input_23[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_70 (Conv2D)             (None, 6, 4763, 16)  528         ['conv2d_69[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 6, 4765, 16)  272         ['conv2d_48[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_53 (Conv2D)             (None, 5, 4762, 16)  1040        ['conv2d_52[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_55 (Conv2D)             (None, 6, 4765, 16)  272         ['conv2d_54[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_59 (Conv2D)             (None, 5, 4762, 16)  1040        ['conv2d_58[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_61 (Conv2D)             (None, 6, 4765, 16)  272         ['conv2d_60[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_65 (Conv2D)             (None, 5, 4762, 16)  1040        ['conv2d_64[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_67 (Conv2D)             (None, 6, 4765, 16)  272         ['conv2d_66[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_71 (Conv2D)             (None, 5, 4762, 16)  1040        ['conv2d_70[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 5, 4765, 16)  528         ['conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_8 (MaxPooling2D)  (None, 1, 4762, 16)  0          ['conv2d_53[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_56 (Conv2D)             (None, 5, 4765, 16)  528         ['conv2d_55[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_9 (MaxPooling2D)  (None, 1, 4762, 16)  0          ['conv2d_59[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_62 (Conv2D)             (None, 5, 4765, 16)  528         ['conv2d_61[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_10 (MaxPooling2D  (None, 1, 4762, 16)  0          ['conv2d_65[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_68 (Conv2D)             (None, 5, 4765, 16)  528         ['conv2d_67[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_11 (MaxPooling2D  (None, 1, 4762, 16)  0          ['conv2d_71[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " flatten_16 (Flatten)           (None, 381200)       0           ['conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_17 (Flatten)           (None, 76192)        0           ['max_pooling2d_8[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_18 (Flatten)           (None, 381200)       0           ['conv2d_56[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_19 (Flatten)           (None, 76192)        0           ['max_pooling2d_9[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_20 (Flatten)           (None, 381200)       0           ['conv2d_62[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_21 (Flatten)           (None, 76192)        0           ['max_pooling2d_10[0][0]']       \n",
            "                                                                                                  \n",
            " flatten_22 (Flatten)           (None, 381200)       0           ['conv2d_68[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_23 (Flatten)           (None, 76192)        0           ['max_pooling2d_11[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_68 (Dropout)           (None, 381200)       0           ['flatten_16[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_71 (Dropout)           (None, 76192)        0           ['flatten_17[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_76 (Dropout)           (None, 381200)       0           ['flatten_18[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_79 (Dropout)           (None, 76192)        0           ['flatten_19[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_84 (Dropout)           (None, 381200)       0           ['flatten_20[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_87 (Dropout)           (None, 76192)        0           ['flatten_21[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_92 (Dropout)           (None, 381200)       0           ['flatten_22[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_95 (Dropout)           (None, 76192)        0           ['flatten_23[0][0]']             \n",
            "                                                                                                  \n",
            " dense_80 (Dense)               (None, 8)            3049608     ['dropout_68[0][0]']             \n",
            "                                                                                                  \n",
            " dense_83 (Dense)               (None, 8)            609544      ['dropout_71[0][0]']             \n",
            "                                                                                                  \n",
            " dense_89 (Dense)               (None, 8)            3049608     ['dropout_76[0][0]']             \n",
            "                                                                                                  \n",
            " dense_92 (Dense)               (None, 8)            609544      ['dropout_79[0][0]']             \n",
            "                                                                                                  \n",
            " dense_98 (Dense)               (None, 8)            3049608     ['dropout_84[0][0]']             \n",
            "                                                                                                  \n",
            " dense_101 (Dense)              (None, 8)            609544      ['dropout_87[0][0]']             \n",
            "                                                                                                  \n",
            " dense_107 (Dense)              (None, 8)            3049608     ['dropout_92[0][0]']             \n",
            "                                                                                                  \n",
            " dense_110 (Dense)              (None, 8)            609544      ['dropout_95[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_69 (Dropout)           (None, 8)            0           ['dense_80[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_72 (Dropout)           (None, 8)            0           ['dense_83[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_77 (Dropout)           (None, 8)            0           ['dense_89[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_80 (Dropout)           (None, 8)            0           ['dense_92[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_85 (Dropout)           (None, 8)            0           ['dense_98[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_88 (Dropout)           (None, 8)            0           ['dense_101[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_93 (Dropout)           (None, 8)            0           ['dense_107[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_96 (Dropout)           (None, 8)            0           ['dense_110[0][0]']              \n",
            "                                                                                                  \n",
            " dense_81 (Dense)               (None, 4)            36          ['dropout_69[0][0]']             \n",
            "                                                                                                  \n",
            " dense_84 (Dense)               (None, 4)            36          ['dropout_72[0][0]']             \n",
            "                                                                                                  \n",
            " dense_90 (Dense)               (None, 4)            36          ['dropout_77[0][0]']             \n",
            "                                                                                                  \n",
            " dense_93 (Dense)               (None, 4)            36          ['dropout_80[0][0]']             \n",
            "                                                                                                  \n",
            " dense_99 (Dense)               (None, 4)            36          ['dropout_85[0][0]']             \n",
            "                                                                                                  \n",
            " dense_102 (Dense)              (None, 4)            36          ['dropout_88[0][0]']             \n",
            "                                                                                                  \n",
            " dense_108 (Dense)              (None, 4)            36          ['dropout_93[0][0]']             \n",
            "                                                                                                  \n",
            " dense_111 (Dense)              (None, 4)            36          ['dropout_96[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_70 (Dropout)           (None, 4)            0           ['dense_81[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_73 (Dropout)           (None, 4)            0           ['dense_84[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_78 (Dropout)           (None, 4)            0           ['dense_90[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_81 (Dropout)           (None, 4)            0           ['dense_93[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_86 (Dropout)           (None, 4)            0           ['dense_99[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_89 (Dropout)           (None, 4)            0           ['dense_102[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_94 (Dropout)           (None, 4)            0           ['dense_108[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_97 (Dropout)           (None, 4)            0           ['dense_111[0][0]']              \n",
            "                                                                                                  \n",
            " dense_82 (Dense)               (None, 2)            10          ['dropout_70[0][0]']             \n",
            "                                                                                                  \n",
            " dense_85 (Dense)               (None, 2)            10          ['dropout_73[0][0]']             \n",
            "                                                                                                  \n",
            " dense_91 (Dense)               (None, 2)            10          ['dropout_78[0][0]']             \n",
            "                                                                                                  \n",
            " dense_94 (Dense)               (None, 2)            10          ['dropout_81[0][0]']             \n",
            "                                                                                                  \n",
            " dense_100 (Dense)              (None, 2)            10          ['dropout_86[0][0]']             \n",
            "                                                                                                  \n",
            " dense_103 (Dense)              (None, 2)            10          ['dropout_89[0][0]']             \n",
            "                                                                                                  \n",
            " dense_109 (Dense)              (None, 2)            10          ['dropout_94[0][0]']             \n",
            "                                                                                                  \n",
            " dense_112 (Dense)              (None, 2)            10          ['dropout_97[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 4)            0           ['dense_82[0][0]',               \n",
            "                                                                  'dense_85[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 4)            0           ['dense_91[0][0]',               \n",
            "                                                                  'dense_94[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenate)   (None, 4)            0           ['dense_100[0][0]',              \n",
            "                                                                  'dense_103[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenate)   (None, 4)            0           ['dense_109[0][0]',              \n",
            "                                                                  'dense_112[0][0]']              \n",
            "                                                                                                  \n",
            " dense_86 (Dense)               (None, 8)            40          ['concatenate_10[0][0]']         \n",
            "                                                                                                  \n",
            " dense_95 (Dense)               (None, 8)            40          ['concatenate_11[0][0]']         \n",
            "                                                                                                  \n",
            " dense_104 (Dense)              (None, 8)            40          ['concatenate_12[0][0]']         \n",
            "                                                                                                  \n",
            " dense_113 (Dense)              (None, 8)            40          ['concatenate_13[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_74 (Dropout)           (None, 8)            0           ['dense_86[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_82 (Dropout)           (None, 8)            0           ['dense_95[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_90 (Dropout)           (None, 8)            0           ['dense_104[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_98 (Dropout)           (None, 8)            0           ['dense_113[0][0]']              \n",
            "                                                                                                  \n",
            " dense_87 (Dense)               (None, 8)            72          ['dropout_74[0][0]']             \n",
            "                                                                                                  \n",
            " dense_96 (Dense)               (None, 8)            72          ['dropout_82[0][0]']             \n",
            "                                                                                                  \n",
            " dense_105 (Dense)              (None, 8)            72          ['dropout_90[0][0]']             \n",
            "                                                                                                  \n",
            " dense_114 (Dense)              (None, 8)            72          ['dropout_98[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_75 (Dropout)           (None, 8)            0           ['dense_87[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_83 (Dropout)           (None, 8)            0           ['dense_96[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_91 (Dropout)           (None, 8)            0           ['dense_105[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_99 (Dropout)           (None, 8)            0           ['dense_114[0][0]']              \n",
            "                                                                                                  \n",
            " dense_88 (Dense)               (None, 8)            72          ['dropout_75[0][0]']             \n",
            "                                                                                                  \n",
            " dense_97 (Dense)               (None, 8)            72          ['dropout_83[0][0]']             \n",
            "                                                                                                  \n",
            " dense_106 (Dense)              (None, 8)            72          ['dropout_91[0][0]']             \n",
            "                                                                                                  \n",
            " dense_115 (Dense)              (None, 8)            72          ['dropout_99[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenate)   (None, 32)           0           ['dense_88[0][0]',               \n",
            "                                                                  'dense_97[0][0]',               \n",
            "                                                                  'dense_106[0][0]',              \n",
            "                                                                  'dense_115[0][0]']              \n",
            "                                                                                                  \n",
            " dense_118 (Dense)              (None, 8)            264         ['concatenate_14[0][0]']         \n",
            "                                                                                                  \n",
            " dense_119 (Dense)              (None, 2)            18          ['dense_118[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 14,647,850\n",
            "Trainable params: 14,647,850\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<b>Building the dataset generators to feed the Neural Net</b>"
      ],
      "metadata": {
        "id": "OeXDr3wfZLc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trimmer(datas):\n",
        "  l = []\n",
        "  for n in datas.keys():\n",
        "    l += [len(datas[n]['X1'])]\n",
        "  prune_length = min([len(d['X1']) for d in datas.values()])\n",
        "  for n in datas.keys():\n",
        "    new = {}\n",
        "    for k in datas[n].keys():\n",
        "      new[k] = datas[n][k][:prune_length]\n",
        "    datas[n] = new.copy()\n",
        "  return\n",
        "\n",
        "def recomputer(L):  \n",
        "  global BATCH\n",
        "  LTR = int(L*(1-VAL))\n",
        "  print(f\"Old batch size was: {BATCH}\")\n",
        "  B = 32\n",
        "  LTR = LTR//B * B # LTR is approximated to the closest multiple of B :-) which is convenient for GPU's given their architecture\n",
        "  LVA = L - LTR\n",
        "  LVA = LVA // B * B # THe same for LVA\n",
        "  BATCH = (BATCH // B + 1) * B # And the same for Batch Size\n",
        "  print(f\"New batch size is: {BATCH}, LVA and LTR are: {LVA}, {LTR} and L is {L}, ... does L>=LTR+LVA? {L>=(LTR+LVA)}\")\n",
        "  return BATCH, L, LVA, LTR\n",
        "\n",
        "\n",
        "def get_refs_for(data):\n",
        "  global T\n",
        "  global SAMPLES\n",
        "  y = np.asarray(data['Y'])\n",
        "  ixs = np.asarray(range(len(y)))\n",
        "  pos = ixs[y==1]\n",
        "  neg = ixs[y==0]\n",
        "\n",
        "  minL = min([len(pos), len(neg)])\n",
        "\n",
        "  # minL/(T+SAMPLES)=SLICES\n",
        "  WIDTH = (T+SAMPLES)\n",
        "  SLICES = minL//WIDTH\n",
        "  assert(SLICES>0)\n",
        "\n",
        "  # Produce the slices\n",
        "  posSlices = [pos[i*WIDTH : (i+1)*WIDTH] for i in range(SLICES)]\n",
        "  negSlices = [neg[i*WIDTH : (i+1)*WIDTH] for i in range(SLICES)]\n",
        "\n",
        "  # Filter only connected slices :-)\n",
        "  TARGET = [0] * (T+SAMPLES-1)\n",
        "  posSlices = [x for x in posSlices if (np.diff(x)-1).tolist() == TARGET]\n",
        "  negSlices = [x for x in negSlices if (np.diff(x)-1).tolist() == TARGET]\n",
        "\n",
        "  # Enforce a tolerance of e.g. 3% difference in length, i.e. how unbalanced the dataset will be.\n",
        "  TOL = max([int(0.03*len(posSlices)),1])\n",
        "  assert(abs(len(posSlices) - len(negSlices)) <= TOL)\n",
        "\n",
        "  # Produce more effective indexes\n",
        "  posRefs = []\n",
        "  for x in posSlices:\n",
        "    posRefs += [x[i+T] for i in range(SAMPLES)]\n",
        "  negRefs = []\n",
        "  for x in negSlices:\n",
        "    negRefs += [x[i+T] for i in range(SAMPLES)]\n",
        "\n",
        "  # Shuffle and return :-)\n",
        "  np.random.shuffle(negRefs)\n",
        "  np.random.shuffle(posRefs)\n",
        "  return negRefs, posRefs\n",
        "\n",
        "\n",
        "def build_data(datas,n):\n",
        "  global MAXPAD\n",
        "  # Open data\n",
        "  data = datas[n]\n",
        "  X1, X20, Y = data['X1'],data['X2'],data['Y']\n",
        "\n",
        "  # Process Y\n",
        "  ONE_HOT_Y = np.zeros((len(Y),2))\n",
        "  for i in range(len(Y)):\n",
        "      ONE_HOT_Y[i,Y[i]] = 1\n",
        "  ONE_HOT_Y = ONE_HOT_Y.astype('float32')\n",
        "\n",
        "  # Process X1\n",
        "  X1_TRACKER = []\n",
        "  for i_,X_ in enumerate(X1):\n",
        "      X1_TRACKER.append((MAXPAD-len(X_)))\n",
        "      X1[i_] = [float(x) / UNIQUES for x in X_]\n",
        "\n",
        "  # Process X2\n",
        "  X2 = [[] for _ in range(len(X20))]\n",
        "  X2_TRACKER = []\n",
        "  for i,X_ in enumerate(X20):\n",
        "      c = 0\n",
        "      for i_,y_ in enumerate(X_):\n",
        "          for z in y_:\n",
        "              X2[i] += [[float(i_)/MAXPAD,float(z)/MAXPAD]]\n",
        "              c += 1\n",
        "      X2_TRACKER.append(MAXPAD-c) \n",
        "\n",
        "  # Record the results\n",
        "  datas[n]['X1'] = (X1, X1_TRACKER)\n",
        "  datas[n]['X2'] = (X2, X2_TRACKER)\n",
        "  datas[n]['Y'] = (ONE_HOT_Y,)\n",
        "\n",
        "  return\n",
        "\n",
        "\n",
        "def produce_data(A,B,Refs):\n",
        "    counter = 0  \n",
        "    w = list(range(A,B))\n",
        "    np.random.shuffle(w)\n",
        "    YS = datas[PROCS[0]]['Y'][0]\n",
        "    LIMIT  = 2 * len(Refs[0])\n",
        "    while counter < LIMIT:\n",
        "            i = Refs[counter % 2][A + counter // 2]\n",
        "            yield (\n",
        "                    tuple([\n",
        "                  ( tf.convert_to_tensor(np.asarray([ (datas[nproc]['X1'][0][j] + [0.] * datas[nproc]['X1'][1][j]) for j in range(i-T+1,i+1)]).reshape(1,T,-1,1)), \n",
        "                tf.convert_to_tensor(np.asarray([ (datas[nproc]['X2'][0][j] + [[0., 0.]] * datas[nproc]['X2'][1][j]) for j in range(i-T+1,i+1)]).reshape(1,T,-1,2)),\n",
        "              ) for nproc in range(len(PROCS))\n",
        "                        ]), \n",
        "                    tf.convert_to_tensor(YS[i:i+1,:].reshape(1,2)),\n",
        "                  )\n",
        "            counter += 1\n",
        "            \n",
        "\n",
        "trimmer(datas)\n",
        "Refs  = get_refs_for(datas[1])\n",
        "BATCH, L, LVA, LTR = recomputer(len(Refs[0]))\n",
        "print(f'Len of refs is {len(Refs[0])}, L,LVA,LTR are {L},{LVA},{LTR}')\n",
        "for n in PROCS:\n",
        "  build_data(datas,n)\n",
        "\n",
        "\n",
        "def produce_entire_datasets(L0,Lref, name=''):\n",
        "  \"\"\"\n",
        "  This function attempts to fit the entire dataset into memory.\n",
        "  For a 30-secs kernel trace, approx 10 Mb of pickle data per processor, it fails.\n",
        "  The key is that it includes padding ;-)\n",
        "  \"\"\"\n",
        "  global Refs\n",
        "  it = produce_data(L0,Lref, Refs)\n",
        "  data = []\n",
        "  counter = 0\n",
        "  while True:\n",
        "    try:\n",
        "      data += [it.__next__()]\n",
        "    except:\n",
        "      break\n",
        "    counter += 1\n",
        "  return data\n",
        "\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "    if isinstance(value, type(tf.constant(0))): # if value ist tensor\n",
        "        value = value.numpy() # get value of tensor\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "FEATURE_NAME = []\n",
        "for i in range(len(PROCS)):\n",
        "  FEATURE_NAME += [f\"X0_P{i}\", f\"X1_P{i}\"]\n",
        "FEATURE_NAME += [\"Y\"]\n",
        "parse_dic = {\n",
        "    fname: tf.io.FixedLenFeature([], tf.string) for fname in FEATURE_NAME\n",
        "    }\n",
        "def _parse_tfr_element(element):\n",
        "  example_message = tf.io.parse_single_example(element, parse_dic)\n",
        "  byteFeatures = [example_message.get(FEATURE_NAME[i],[]) for i in range(BASE)]\n",
        "  features = [tf.io.parse_tensor(x, out_type=tf.float32) for x in byteFeatures]\n",
        "  return features    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ykTxt9Ll-8Y",
        "outputId": "33df2a29-1f11-47f6-e279-2b4f261539a8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old batch size was: 128\n",
            "New batch size is: 160, LVA and LTR are: 448, 1344 and L is 1800, ... does L>=LTR+LVA? True\n",
            "Len of refs is 1800, L,LVA,LTR are 1800,448,1344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<b>Training the Neural Net</b>"
      ],
      "metadata": {
        "id": "oteqJUB2ZR54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_ITERATOR_APPROACH = [True,False][1]"
      ],
      "metadata": {
        "id": "RPQo6KISZ6-1"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DATASET_ITERATOR_APPROACH:\n",
        "  OS = (\n",
        "          tuple([\n",
        "            (tf.TensorSpec(shape=(None,T,MAXPAD,1), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None,T,MAXPAD,2), dtype=tf.float32)\n",
        "            ) for nproc in range(len(PROCS)) \n",
        "            ]),\n",
        "            tf.TensorSpec(shape=(None,NCATEGORIES), dtype=tf.float32),\n",
        "      )\n",
        "  print(f'Finished building the output structure')\n",
        "  trainD = tf.data.Dataset.from_generator(lambda: produce_data(0,LTR, Refs), output_signature=OS)#output_types=(tf.float32), output_shapes=OS)\n",
        "  print(f'Finished building the training dataset')\n",
        "  valD = tf.data.Dataset.from_generator(lambda: produce_data(LTR,L, Refs), output_signature=OS)# output_types=(tf.float32), output_shapes=OS)\n",
        "  print(f'Finished building the validation dataset')\n",
        "  print(f'About to train! :-)')\n",
        "  history = model.fit(trainD, epochs=10, batch_size=BATCH, validation_data=valD, verbose=2)\n",
        "  print('Finished training! :-)')"
      ],
      "metadata": {
        "id": "LPBNnZ9HRXHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not DATASET_ITERATOR_APPROACH:\n",
        "\n",
        "  # Spawn data iterators\n",
        "  trainD = produce_data(0,LTR, Refs)\n",
        "  valD = produce_data(LTR,L, Refs)\n",
        "    \n",
        "  # Prepare variables, configuration, containers, and write the entire\n",
        "  # dataset to a file that can be progressively piped later. \n",
        "  # The point is that TensorFlow can prefetch and/or use parallelization\n",
        "  # to loop more efficiently, as opposed to the GIL impacting on the \n",
        "  # performance of the dataset from generator in the typical training :-)\n",
        "  train_file_paths = [f'data.tfrecords-train{i}' for i in range(4)]\n",
        "  THR_TRAINF = LTR//4\n",
        "  val_file_paths = [f'data.tfrecords-val{i}' for i in range(4)]\n",
        "  THR_VALF = (L-LTR)//4\n",
        "  BASE = 2 * len(PROCS) + 1\n",
        "\n",
        "  TEST = True\n",
        "  KEEP_GOING = True\n",
        "  counter = 0\n",
        "  PERIOD1 = 50\n",
        "  data = []\n",
        "  nameix = 0\n",
        "  while KEEP_GOING:\n",
        "    if (counter+1)%THR_TRAINF == 0:\n",
        "      if nameix==len(train_file_paths)-1: pass\n",
        "      else: nameix += 1\n",
        "    try:\n",
        "      data += [trainD.__next__()]\n",
        "      if TEST:\n",
        "        if counter > 30: raise Exception(\"This is only a test ;-)\")\n",
        "    except:\n",
        "      KEEP_GOING = False\n",
        "      counter = PERIOD1-1\n",
        "    counter += 1\n",
        "    if counter % PERIOD1 == 0:\n",
        "      arrays = []\n",
        "      for dat in data:\n",
        "        local_array = []\n",
        "        for P in dat[0]:\n",
        "          local_array += [tf.io.serialize_tensor(P[0].astype('float32')), tf.io.serialize_tensor(P[1].astype('float32'))]\n",
        "        local_array += [tf.io.serialize_tensor(dat[1])]\n",
        "        arrays.append(local_array.copy())\n",
        "      with tf.io.TFRecordWriter(train_file_paths[nameix]) as writer:\n",
        "        for serialized_arrays in arrays:\n",
        "          features = {FEATURE_NAME[c]: _bytes_feature(serialized_array) for c,serialized_array in enumerate(serialized_arrays)}\n",
        "          example_message = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "          writer.write(example_message.SerializeToString())\n",
        "      del data\n",
        "      data = []\n",
        "  data = []\n",
        "  KEEP_GOING = True\n",
        "  counter = 0\n",
        "  nameix = 0\n",
        "  while KEEP_GOING:\n",
        "    if (counter+1)%THR_VALF == 0:\n",
        "      if nameix==len(val_file_paths)-1: pass\n",
        "      else: nameix += 1\n",
        "    try:\n",
        "      data += [valD.__next__()]\n",
        "      if TEST:\n",
        "        if counter > 30: raise Exception(\"This is only a test ;-)\")\n",
        "    except:\n",
        "      KEEP_GOING = False\n",
        "      counter = PERIOD1-1\n",
        "    counter += 1\n",
        "    if counter % PERIOD1 == 0:\n",
        "      arrays = []\n",
        "      for dat in data:\n",
        "        local_array = []\n",
        "        for P in dat[0]:\n",
        "          local_array += [tf.io.serialize_tensor(P[0].astype('float32')), tf.io.serialize_tensor(P[1].astype('float32'))]\n",
        "        local_array += [tf.io.serialize_tensor(dat[1])]\n",
        "        arrays.append(local_array.copy())\n",
        "      with tf.io.TFRecordWriter(val_file_paths[nameix]) as writer:\n",
        "        for serialized_arrays in arrays:\n",
        "          features = {FEATURE_NAME[c]: _bytes_feature(serialized_array) for c,serialized_array in enumerate(serialized_arrays)}\n",
        "          example_message = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "          writer.write(example_message.SerializeToString())\n",
        "      del data\n",
        "      data = []      "
      ],
      "metadata": {
        "id": "Jdr7WzJmZ37f"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not DATASET_ITERATOR_APPROACH:\n",
        "\n",
        "  print(f'About to train! :-)') \n",
        "\n",
        "  tfr_dataset = tf.data.TFRecordDataset(train_file_paths, num_parallel_reads=4) \n",
        "  dataset = tfr_dataset.map(_parse_tfr_element)\n",
        "  tfr_val_dataset = tf.data.TFRecordDataset(val_file_paths, num_parallel_reads=4) \n",
        "  val_dataset = tfr_dataset.map(_parse_tfr_element)\n",
        "  for E in range(EPOCHS):\n",
        "    t0 = time.time()\n",
        "    Y = []\n",
        "    for i,x in enumerate(dataset):\n",
        "      a,b,c,d,e,f,g,h,k = x\n",
        "      # We can still add a layer of control by saving the data with their batches and all :-) UPGRADE\n",
        "      if i==0:\n",
        "        X0a = a\n",
        "        X0b = b\n",
        "        X1a = c\n",
        "        X1b = d\n",
        "        X2a = e\n",
        "        X2b = f\n",
        "        X3a = g\n",
        "        X3b = h\n",
        "      else:\n",
        "        X0a = tf.concat([X0a,a],0)\n",
        "        X0b = tf.concat([X0b,b],0)\n",
        "        X1a = tf.concat([X1a,c],0)\n",
        "        X1b = tf.concat([X1b,d],0)\n",
        "        X2a = tf.concat([X2a,e],0)\n",
        "        X2b = tf.concat([X2b,f],0)\n",
        "        X3a = tf.concat([X3a,g],0)\n",
        "        X3b = tf.concat([X3b,h],0)\n",
        "      Y.append(k[0])\n",
        "      if (i+1)%B==0:\n",
        "        xdata = [X0a,X0b,X1a,X1b,X2a,X2b,X3a,X3b]\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(xdata, training=True) \n",
        "            loss_value = model.loss(Y, logits)\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    for i,x in enumerate(dataset):\n",
        "      # Collect data with a mechanism as in the previous section \n",
        "      #logits = model(xdata, training=True)\n",
        "      #model.metrics.compute or something\n",
        "      # Extra bibliography is: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n",
        "      pass\n",
        "\n",
        "    # Time report... is this method really better?   \n",
        "    tf = time.time()\n",
        "    print(f'Time for Epoch {E} and batch size {B} has been {tf-t0} seconds')\n",
        "\n",
        "  print('Finished training! :-)')"
      ],
      "metadata": {
        "id": "niubwlkMbgAF",
        "outputId": "b258f29e-e24c-42ac-cd6d-41b69d074493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "About to train! :-)\n",
            "Finished training! :-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<b>Plotting the Neural Net results</b>"
      ],
      "metadata": {
        "id": "ildO_9EYZVol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if DATASET_ITERATOR_APPROACH:\n",
        "  # Display results\n",
        "  f,ax = plt.subplots(1,2,figsize=(15,10))\n",
        "\n",
        "  ax[0].plot(history.history['loss'],label='loss', c='k', lw=2)\n",
        "  ax[0].plot(history.history['val_loss'], label='val loss', c='r', lw=2)\n",
        "  ax[0].grid()\n",
        "  ax[0].set_title('Training and Validation Loss* over the Epochs\\n*TensorFlow categorical crossentropy')\n",
        "  ax[0].set_ylim(0,None)\n",
        "  ax[0].legend()\n",
        "\n",
        "  ax[1].plot(history.history['categorical_accuracy'],label='accuracy', c='k', lw=2)\n",
        "  ax[1].plot(history.history['val_categorical_accuracy'], label='val accuracy', c='r', lw=2)\n",
        "  ax[1].set_ylim(0,1)\n",
        "  ax[1].hlines(0.5, 0, EPOCHS, color='y', ls=':', lw=4, label='Null Hypothesis\\n(perfect 50% tag balance case)')\n",
        "  ax[1].grid()\n",
        "  ax[1].set_title('Training and Validation Acurracy over the Epochs')\n",
        "  ax[1].legend()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "rktl-jpbmLRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DATASET_ITERATOR_APPROACH:\n",
        "  # Print the raw values of the metrics, just in case we want to use them without re-running the notebook :O\n",
        "  print(history.history['loss'])\n",
        "  print(history.history['val_loss'])\n",
        "  print(history.history['categorical_accuracy'])\n",
        "  print(history.history['val_categorical_accuracy'])"
      ],
      "metadata": {
        "id": "j0VBVFx0nMh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Dy95bCPsYGrT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}