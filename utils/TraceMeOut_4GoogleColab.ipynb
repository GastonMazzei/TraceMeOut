{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TraceMeOut-4GoogleColab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Required Imports</b>"
      ],
      "metadata": {
        "id": "S6WiCYxRYJMy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NJdGVNSkPlx",
        "outputId": "a4e94220-6e6b-44e5-851f-47027d3399df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ],
      "source": [
        "import pickle,sys,os, time\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "BYPASS_PREVIOUS_SECTION = [True, False][0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset0.pkl?raw=true\n",
        "!wget https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset1.pkl?raw=true\n",
        "!wget https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset2.pkl?raw=true\n",
        "!wget https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset3.pkl?raw=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRjDxDxSlffM",
        "outputId": "2e4873b6-80b2-4902-fffe-36bb98115b14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-31 16:27:40--  https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset0.pkl?raw=true\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset0.pkl [following]\n",
            "--2021-12-31 16:27:40--  https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset0.pkl\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset0.pkl [following]\n",
            "--2021-12-31 16:27:40--  https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset0.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18299973 (17M) [application/octet-stream]\n",
            "Saving to: ‘Dataset0.pkl?raw=true’\n",
            "\n",
            "Dataset0.pkl?raw=tr 100%[===================>]  17.45M   114MB/s    in 0.2s    \n",
            "\n",
            "2021-12-31 16:27:41 (114 MB/s) - ‘Dataset0.pkl?raw=true’ saved [18299973/18299973]\n",
            "\n",
            "--2021-12-31 16:27:41--  https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset1.pkl?raw=true\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset1.pkl [following]\n",
            "--2021-12-31 16:27:41--  https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset1.pkl\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset1.pkl [following]\n",
            "--2021-12-31 16:27:41--  https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset1.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24479477 (23M) [application/octet-stream]\n",
            "Saving to: ‘Dataset1.pkl?raw=true’\n",
            "\n",
            "Dataset1.pkl?raw=tr 100%[===================>]  23.34M   126MB/s    in 0.2s    \n",
            "\n",
            "2021-12-31 16:27:43 (126 MB/s) - ‘Dataset1.pkl?raw=true’ saved [24479477/24479477]\n",
            "\n",
            "--2021-12-31 16:27:43--  https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset2.pkl?raw=true\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset2.pkl [following]\n",
            "--2021-12-31 16:27:43--  https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset2.pkl\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset2.pkl [following]\n",
            "--2021-12-31 16:27:43--  https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset2.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12815767 (12M) [application/octet-stream]\n",
            "Saving to: ‘Dataset2.pkl?raw=true’\n",
            "\n",
            "Dataset2.pkl?raw=tr 100%[===================>]  12.22M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-12-31 16:27:44 (119 MB/s) - ‘Dataset2.pkl?raw=true’ saved [12815767/12815767]\n",
            "\n",
            "--2021-12-31 16:27:44--  https://github.com/GastonMazzei/TraceMeOut/blob/main/processed_trace/Dataset3.pkl?raw=true\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset3.pkl [following]\n",
            "--2021-12-31 16:27:45--  https://github.com/GastonMazzei/TraceMeOut/raw/main/processed_trace/Dataset3.pkl\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset3.pkl [following]\n",
            "--2021-12-31 16:27:45--  https://raw.githubusercontent.com/GastonMazzei/TraceMeOut/main/processed_trace/Dataset3.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42367432 (40M) [application/octet-stream]\n",
            "Saving to: ‘Dataset3.pkl?raw=true’\n",
            "\n",
            "Dataset3.pkl?raw=tr 100%[===================>]  40.40M  75.2MB/s    in 0.5s    \n",
            "\n",
            "2021-12-31 16:27:47 (75.2 MB/s) - ‘Dataset3.pkl?raw=true’ saved [42367432/42367432]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeBPa_RrlnaD",
        "outputId": "c44d585f-824a-4788-aa1b-42f7bbbdc1c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Dataset0.pkl?raw=true'  'Dataset2.pkl?raw=true'   sample_data\n",
            "'Dataset1.pkl?raw=true'  'Dataset3.pkl?raw=true'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not BYPASS_PREVIOUS_SECTION:\n",
        "  datas = {}\n",
        "  for n in [0,1,2,3]:\n",
        "    with open(f'Dataset{n}.pkl?raw=true','rb') as f:\n",
        "      datas[n] = pickle.load(f)"
      ],
      "metadata": {
        "id": "_AFenX9QlrWx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<b>Configuration File: architecture & hyperparameters</b>\n",
        "\n",
        "(<i>If the datasets change in the repo, non-architectural information should be updated using the configuration.py file. Most probably an error will be raised because of size mismatch if this is not updated.</i>)\n",
        "<b>IF it's not the first run, and you have accepted to store the data in GoogleColab, you can skip to the next section :-)</b>"
      ],
      "metadata": {
        "id": "ZoqNt7-YYn-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model-specific parameters\n",
        "T=8 # duration of the window in dt units\n",
        "dt = 4000 # time in microseconds\n",
        "UNIQUES=3807  #number of unique ids\n",
        "MI=4762  #max number of interactions\n",
        "ML=4765  #max number of leaves\n",
        "NCATEGORIES=2\n",
        "\n",
        "# Architectural parameters\n",
        "ACT1 = 'relu'\n",
        "FILTERS1 = 8\n",
        "KSIZE1 = (2,1)\n",
        "PSIZE1 = (max([T//4,2]),)\n",
        "NDENSE1 = 8\n",
        "DROP1 = 0.6\n",
        "\n",
        "ACT2 = 'relu'\n",
        "FILTERS2 = 8\n",
        "KSIZE2 = (2,2)\n",
        "PSIZE2 = (max([T//2,2]),1)\n",
        "stride = (1,1)\n",
        "NDENSE2 = 8\n",
        "DROP2 = 0.7\n",
        "\n",
        "ACT3='relu'\n",
        "NDENSE3=8\n",
        "DROP3 = 0.5\n",
        "\n",
        "\n",
        "ACT4='relu'\n",
        "NDENSE4=8\n",
        "DROP4 = 0.5\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "VAL=0.25\n",
        "BATCH=128\n",
        "EPOCHS=20\n",
        "L=5 # a length used to generate random data just for testing\n",
        "LR=0.02\n",
        "SAMPLES = 30\n",
        "\n",
        "\n",
        "# Extras\n",
        "POOLING = False\n",
        "PROCS=[3, 2, 0, 1]"
      ],
      "metadata": {
        "id": "nsC-H7jvkmMn"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assert the dataset's keys and the procs in the configuration are the same :-)\n",
        "temp1, temp2 = sorted(PROCS), sorted(list(datas.keys()))\n",
        "assert(temp1 == temp2)\n",
        "NPROCS = len(PROCS)\n",
        "\n",
        "# define two sets of inputs: \n",
        "MAXPAD = max([MI,ML])\n",
        "input_shape_flavours = (BATCH, T, MAXPAD, 1)\n",
        "input_shape_structure = (BATCH, T, MAXPAD, 2)\n",
        "\n",
        "def produce_model():\n",
        "\tinputFlavours = tf.keras.Input(shape=input_shape_flavours[1:])\n",
        "\tinputStructure = tf.keras.Input(shape=input_shape_structure[1:])\n",
        "\n",
        "\t# the first branch operates on the first input (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)\n",
        "\tx = tf.keras.layers.Conv2D(\n",
        "\t\t\t\t# Filters, Kersize, Strides, Padding,  Activation\n",
        "\t\t\t\tFILTERS1,         KSIZE1,       (1,1),      'valid',  activation = ACT1,\n",
        "\t\t\t\tinput_shape = input_shape_flavours[1:]\n",
        "\t\t\t\t)(inputFlavours)\n",
        "\tx = tf.keras.layers.Conv2D(\n",
        "\t\t\t\t# Filters, Kersize, Strides, Padding,  Activation\n",
        "\t\t\t\tFILTERS1 * 2,         KSIZE1,       (1,1),      'valid',  activation = ACT1,\n",
        "\t\t\t\t)(x)\n",
        "\tx = tf.keras.layers.Conv2D(\n",
        "\t\t\t\t# Filters, Kersize, Strides, Padding,  Activation\n",
        "\t\t\t\tFILTERS1 * 2,         KSIZE1,       (1,1),      'valid',  activation = ACT1,\n",
        "\t\t\t\t)(x)\n",
        "\t#x = tf.keras.layers.MaxPool1D(pool_size=PSIZE1)(x)\n",
        "\tx = tf.keras.layers.Flatten()(x)\n",
        "\tx = tf.keras.layers.Dropout(DROP1)(x)\n",
        "\tx = tf.keras.layers.Dense(NDENSE1, activation = ACT1)(x)\n",
        "\tx = tf.keras.layers.Dropout(DROP1)(x)\n",
        "\tx = tf.keras.layers.Dense(NDENSE1 // 2, activation = ACT1)(x)\n",
        "\tx = tf.keras.layers.Dropout(DROP1)(x)\n",
        "\tx = tf.keras.layers.Dense(NDENSE1 // 2 // 2, activation = ACT1)(x)\n",
        "\tx = tf.keras.Model(inputs = inputFlavours, outputs=x)\n",
        "\n",
        "\t# the second branch opreates on the second input (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)\n",
        "\ty = tf.keras.layers.Conv2D(\n",
        "\t\t\t\t# Filters, Kersize, Strides, Padding,  Activation\n",
        "\t\t\t\tFILTERS2,         KSIZE2,    stride,      'valid',  activation = ACT2,\n",
        "\t\t\t\tinput_shape = input_shape_structure[1:]\n",
        "\t\t\t\t)(inputStructure)\n",
        "\ty = tf.keras.layers.Conv2D(\n",
        "\t\t\t\t# Filters, Kersize, Strides, Padding,  Activation\n",
        "\t\t\t\tFILTERS2 * 2,         KSIZE2,       stride,      'valid',  activation = ACT2,\n",
        "\t\t\t\t)(y)\n",
        "\t#y = tf.keras.layers.MaxPool2D(pool_size=PSIZE2)(y)\n",
        "\ty = tf.keras.layers.Conv2D(\n",
        "\t\t\t\t# Filters, Kersize, Strides, Padding,  Activation\n",
        "\t\t\t\tFILTERS2 * 2,         KSIZE2,       stride,      'valid',  activation = ACT2,\n",
        "\t\t\t\t)(y)\n",
        "\ty = tf.keras.layers.MaxPool2D(pool_size=PSIZE2)(y)\n",
        "\ty = tf.keras.layers.Flatten()(y)\n",
        "\ty = tf.keras.layers.Dropout(DROP2)(y)\n",
        "\ty = tf.keras.layers.Dense(NDENSE2, activation = ACT2)(y)\n",
        "\ty = tf.keras.layers.Dropout(DROP2)(y)\n",
        "\ty = tf.keras.layers.Dense(NDENSE2 // 2, activation = ACT2)(y)\n",
        "\ty = tf.keras.layers.Dropout(DROP2)(y)\n",
        "\ty = tf.keras.layers.Dense(NDENSE2 // 2 // 2, activation = ACT2)(y)\n",
        "\ty = tf.keras.Model(inputs = inputStructure, outputs=y)\n",
        "\n",
        "\n",
        "\n",
        "\t# combine the output of the two branches\n",
        "\tcombined = tf.keras.layers.concatenate([x.output, y.output])\n",
        "\tz = tf.keras.layers.Dense(NDENSE3, activation = ACT3)(combined)\n",
        "\tz = tf.keras.layers.Dropout(DROP3)(z)\n",
        "\tz = tf.keras.layers.Dense(NDENSE3, activation = ACT3)(z)\n",
        "\tz = tf.keras.layers.Dropout(DROP3)(z)\n",
        "\tz = tf.keras.layers.Dense(NDENSE3, activation = ACT3)(z)\n",
        "\n",
        "\t# our model will accept the inputs of the two branches and\n",
        "\t# then output a single value\n",
        "\treturn tf.keras.Model(inputs=[x.input, y.input], outputs=z)\n",
        "\n",
        "models = []\n",
        "for n in range(NPROCS):\n",
        "\tmodels += [produce_model()]\n",
        "TOTALINPUTS = []\n",
        "for m in models:\n",
        "\tTOTALINPUTS += [m.inputs[0], m.inputs[1]]\n",
        "combined = tf.keras.layers.concatenate([m.output for m in models])\n",
        "w = tf.keras.layers.Dense(NDENSE4, activation = ACT4)(combined)\n",
        "w = tf.keras.layers.Dropout(DROP4)(w)\n",
        "w = tf.keras.layers.Dense(NDENSE4, activation = ACT4)(combined)\n",
        "w = tf.keras.layers.Dropout(DROP4)(w)\n",
        "w = tf.keras.layers.Dense(NDENSE4, activation = ACT4)(combined)\n",
        "w = tf.keras.layers.Dense(NCATEGORIES, activation=\"softmax\")(w)\n",
        "model = tf.keras.Model(inputs=TOTALINPUTS, outputs=w)\n",
        "\n",
        "# Compile the model :-)\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=LR),\n",
        "\t\t\t\t\t\t\tloss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "\t\t\t\t\t\t\tmetrics=[\n",
        "\t\t\t\t\t\t\t\t\t\t\t#tf.keras.metrics.CategoricalCrossentropy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\ttf.keras.metrics.CategoricalAccuracy(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t#tf.keras.metrics.AUC(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t])\n",
        "\n",
        "# Print input size\n",
        "print(model.summary())\n",
        "IShape = model.input_shape\n",
        "OShape = model.output_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4gcQStkS5q",
        "outputId": "6d97108c-5d74-48b0-de5f-dc284bae1a5d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_51\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_26 (InputLayer)          [(None, 8, 4765, 2)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_28 (InputLayer)          [(None, 8, 4765, 2)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)          [(None, 8, 4765, 2)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_32 (InputLayer)          [(None, 8, 4765, 2)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " input_25 (InputLayer)          [(None, 8, 4765, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)             (None, 7, 4764, 8)   72          ['input_26[0][0]']               \n",
            "                                                                                                  \n",
            " input_27 (InputLayer)          [(None, 8, 4765, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " conv2d_81 (Conv2D)             (None, 7, 4764, 8)   72          ['input_28[0][0]']               \n",
            "                                                                                                  \n",
            " input_29 (InputLayer)          [(None, 8, 4765, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " conv2d_87 (Conv2D)             (None, 7, 4764, 8)   72          ['input_30[0][0]']               \n",
            "                                                                                                  \n",
            " input_31 (InputLayer)          [(None, 8, 4765, 1)  0           []                               \n",
            "                                ]                                                                 \n",
            "                                                                                                  \n",
            " conv2d_93 (Conv2D)             (None, 7, 4764, 8)   72          ['input_32[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_72 (Conv2D)             (None, 7, 4765, 8)   24          ['input_25[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_76 (Conv2D)             (None, 6, 4763, 16)  528         ['conv2d_75[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_78 (Conv2D)             (None, 7, 4765, 8)   24          ['input_27[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_82 (Conv2D)             (None, 6, 4763, 16)  528         ['conv2d_81[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_84 (Conv2D)             (None, 7, 4765, 8)   24          ['input_29[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_88 (Conv2D)             (None, 6, 4763, 16)  528         ['conv2d_87[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_90 (Conv2D)             (None, 7, 4765, 8)   24          ['input_31[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_94 (Conv2D)             (None, 6, 4763, 16)  528         ['conv2d_93[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_73 (Conv2D)             (None, 6, 4765, 16)  272         ['conv2d_72[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_77 (Conv2D)             (None, 5, 4762, 16)  1040        ['conv2d_76[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_79 (Conv2D)             (None, 6, 4765, 16)  272         ['conv2d_78[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_83 (Conv2D)             (None, 5, 4762, 16)  1040        ['conv2d_82[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_85 (Conv2D)             (None, 6, 4765, 16)  272         ['conv2d_84[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_89 (Conv2D)             (None, 5, 4762, 16)  1040        ['conv2d_88[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_91 (Conv2D)             (None, 6, 4765, 16)  272         ['conv2d_90[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_95 (Conv2D)             (None, 5, 4762, 16)  1040        ['conv2d_94[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_74 (Conv2D)             (None, 5, 4765, 16)  528         ['conv2d_73[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_12 (MaxPooling2D  (None, 1, 4762, 16)  0          ['conv2d_77[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_80 (Conv2D)             (None, 5, 4765, 16)  528         ['conv2d_79[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_13 (MaxPooling2D  (None, 1, 4762, 16)  0          ['conv2d_83[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_86 (Conv2D)             (None, 5, 4765, 16)  528         ['conv2d_85[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_14 (MaxPooling2D  (None, 1, 4762, 16)  0          ['conv2d_89[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_92 (Conv2D)             (None, 5, 4765, 16)  528         ['conv2d_91[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_15 (MaxPooling2D  (None, 1, 4762, 16)  0          ['conv2d_95[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " flatten_24 (Flatten)           (None, 381200)       0           ['conv2d_74[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_25 (Flatten)           (None, 76192)        0           ['max_pooling2d_12[0][0]']       \n",
            "                                                                                                  \n",
            " flatten_26 (Flatten)           (None, 381200)       0           ['conv2d_80[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_27 (Flatten)           (None, 76192)        0           ['max_pooling2d_13[0][0]']       \n",
            "                                                                                                  \n",
            " flatten_28 (Flatten)           (None, 381200)       0           ['conv2d_86[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_29 (Flatten)           (None, 76192)        0           ['max_pooling2d_14[0][0]']       \n",
            "                                                                                                  \n",
            " flatten_30 (Flatten)           (None, 381200)       0           ['conv2d_92[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_31 (Flatten)           (None, 76192)        0           ['max_pooling2d_15[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_102 (Dropout)          (None, 381200)       0           ['flatten_24[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_105 (Dropout)          (None, 76192)        0           ['flatten_25[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_110 (Dropout)          (None, 381200)       0           ['flatten_26[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_113 (Dropout)          (None, 76192)        0           ['flatten_27[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_118 (Dropout)          (None, 381200)       0           ['flatten_28[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_121 (Dropout)          (None, 76192)        0           ['flatten_29[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_126 (Dropout)          (None, 381200)       0           ['flatten_30[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_129 (Dropout)          (None, 76192)        0           ['flatten_31[0][0]']             \n",
            "                                                                                                  \n",
            " dense_120 (Dense)              (None, 8)            3049608     ['dropout_102[0][0]']            \n",
            "                                                                                                  \n",
            " dense_123 (Dense)              (None, 8)            609544      ['dropout_105[0][0]']            \n",
            "                                                                                                  \n",
            " dense_129 (Dense)              (None, 8)            3049608     ['dropout_110[0][0]']            \n",
            "                                                                                                  \n",
            " dense_132 (Dense)              (None, 8)            609544      ['dropout_113[0][0]']            \n",
            "                                                                                                  \n",
            " dense_138 (Dense)              (None, 8)            3049608     ['dropout_118[0][0]']            \n",
            "                                                                                                  \n",
            " dense_141 (Dense)              (None, 8)            609544      ['dropout_121[0][0]']            \n",
            "                                                                                                  \n",
            " dense_147 (Dense)              (None, 8)            3049608     ['dropout_126[0][0]']            \n",
            "                                                                                                  \n",
            " dense_150 (Dense)              (None, 8)            609544      ['dropout_129[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_103 (Dropout)          (None, 8)            0           ['dense_120[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_106 (Dropout)          (None, 8)            0           ['dense_123[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_111 (Dropout)          (None, 8)            0           ['dense_129[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_114 (Dropout)          (None, 8)            0           ['dense_132[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_119 (Dropout)          (None, 8)            0           ['dense_138[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_122 (Dropout)          (None, 8)            0           ['dense_141[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_127 (Dropout)          (None, 8)            0           ['dense_147[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_130 (Dropout)          (None, 8)            0           ['dense_150[0][0]']              \n",
            "                                                                                                  \n",
            " dense_121 (Dense)              (None, 4)            36          ['dropout_103[0][0]']            \n",
            "                                                                                                  \n",
            " dense_124 (Dense)              (None, 4)            36          ['dropout_106[0][0]']            \n",
            "                                                                                                  \n",
            " dense_130 (Dense)              (None, 4)            36          ['dropout_111[0][0]']            \n",
            "                                                                                                  \n",
            " dense_133 (Dense)              (None, 4)            36          ['dropout_114[0][0]']            \n",
            "                                                                                                  \n",
            " dense_139 (Dense)              (None, 4)            36          ['dropout_119[0][0]']            \n",
            "                                                                                                  \n",
            " dense_142 (Dense)              (None, 4)            36          ['dropout_122[0][0]']            \n",
            "                                                                                                  \n",
            " dense_148 (Dense)              (None, 4)            36          ['dropout_127[0][0]']            \n",
            "                                                                                                  \n",
            " dense_151 (Dense)              (None, 4)            36          ['dropout_130[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_104 (Dropout)          (None, 4)            0           ['dense_121[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_107 (Dropout)          (None, 4)            0           ['dense_124[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_112 (Dropout)          (None, 4)            0           ['dense_130[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_115 (Dropout)          (None, 4)            0           ['dense_133[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_120 (Dropout)          (None, 4)            0           ['dense_139[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_123 (Dropout)          (None, 4)            0           ['dense_142[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_128 (Dropout)          (None, 4)            0           ['dense_148[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_131 (Dropout)          (None, 4)            0           ['dense_151[0][0]']              \n",
            "                                                                                                  \n",
            " dense_122 (Dense)              (None, 2)            10          ['dropout_104[0][0]']            \n",
            "                                                                                                  \n",
            " dense_125 (Dense)              (None, 2)            10          ['dropout_107[0][0]']            \n",
            "                                                                                                  \n",
            " dense_131 (Dense)              (None, 2)            10          ['dropout_112[0][0]']            \n",
            "                                                                                                  \n",
            " dense_134 (Dense)              (None, 2)            10          ['dropout_115[0][0]']            \n",
            "                                                                                                  \n",
            " dense_140 (Dense)              (None, 2)            10          ['dropout_120[0][0]']            \n",
            "                                                                                                  \n",
            " dense_143 (Dense)              (None, 2)            10          ['dropout_123[0][0]']            \n",
            "                                                                                                  \n",
            " dense_149 (Dense)              (None, 2)            10          ['dropout_128[0][0]']            \n",
            "                                                                                                  \n",
            " dense_152 (Dense)              (None, 2)            10          ['dropout_131[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenate)   (None, 4)            0           ['dense_122[0][0]',              \n",
            "                                                                  'dense_125[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenate)   (None, 4)            0           ['dense_131[0][0]',              \n",
            "                                                                  'dense_134[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 4)            0           ['dense_140[0][0]',              \n",
            "                                                                  'dense_143[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_18 (Concatenate)   (None, 4)            0           ['dense_149[0][0]',              \n",
            "                                                                  'dense_152[0][0]']              \n",
            "                                                                                                  \n",
            " dense_126 (Dense)              (None, 8)            40          ['concatenate_15[0][0]']         \n",
            "                                                                                                  \n",
            " dense_135 (Dense)              (None, 8)            40          ['concatenate_16[0][0]']         \n",
            "                                                                                                  \n",
            " dense_144 (Dense)              (None, 8)            40          ['concatenate_17[0][0]']         \n",
            "                                                                                                  \n",
            " dense_153 (Dense)              (None, 8)            40          ['concatenate_18[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_108 (Dropout)          (None, 8)            0           ['dense_126[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_116 (Dropout)          (None, 8)            0           ['dense_135[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_124 (Dropout)          (None, 8)            0           ['dense_144[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_132 (Dropout)          (None, 8)            0           ['dense_153[0][0]']              \n",
            "                                                                                                  \n",
            " dense_127 (Dense)              (None, 8)            72          ['dropout_108[0][0]']            \n",
            "                                                                                                  \n",
            " dense_136 (Dense)              (None, 8)            72          ['dropout_116[0][0]']            \n",
            "                                                                                                  \n",
            " dense_145 (Dense)              (None, 8)            72          ['dropout_124[0][0]']            \n",
            "                                                                                                  \n",
            " dense_154 (Dense)              (None, 8)            72          ['dropout_132[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_109 (Dropout)          (None, 8)            0           ['dense_127[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_117 (Dropout)          (None, 8)            0           ['dense_136[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_125 (Dropout)          (None, 8)            0           ['dense_145[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_133 (Dropout)          (None, 8)            0           ['dense_154[0][0]']              \n",
            "                                                                                                  \n",
            " dense_128 (Dense)              (None, 8)            72          ['dropout_109[0][0]']            \n",
            "                                                                                                  \n",
            " dense_137 (Dense)              (None, 8)            72          ['dropout_117[0][0]']            \n",
            "                                                                                                  \n",
            " dense_146 (Dense)              (None, 8)            72          ['dropout_125[0][0]']            \n",
            "                                                                                                  \n",
            " dense_155 (Dense)              (None, 8)            72          ['dropout_133[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenate)   (None, 32)           0           ['dense_128[0][0]',              \n",
            "                                                                  'dense_137[0][0]',              \n",
            "                                                                  'dense_146[0][0]',              \n",
            "                                                                  'dense_155[0][0]']              \n",
            "                                                                                                  \n",
            " dense_158 (Dense)              (None, 8)            264         ['concatenate_19[0][0]']         \n",
            "                                                                                                  \n",
            " dense_159 (Dense)              (None, 2)            18          ['dense_158[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 14,647,850\n",
            "Trainable params: 14,647,850\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<b>Building the dataset preprocessors and generators to feed the Neural Net</b>"
      ],
      "metadata": {
        "id": "OeXDr3wfZLc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trimmer(datas):\n",
        "  l = []\n",
        "  for n in datas.keys():\n",
        "    l += [len(datas[n]['X1'])]\n",
        "  prune_length = min([len(d['X1']) for d in datas.values()])\n",
        "  for n in datas.keys():\n",
        "    new = {}\n",
        "    for k in datas[n].keys():\n",
        "      new[k] = datas[n][k][:prune_length]\n",
        "    datas[n] = new.copy()\n",
        "  return\n",
        "\n",
        "def recomputer(L):  \n",
        "  global BATCH\n",
        "  LTR = int(L*(1-VAL))\n",
        "  print(f\"Old batch size was: {BATCH}\")\n",
        "  B = 32\n",
        "  LTR = LTR//B * B # LTR is approximated to the closest multiple of B :-) which is convenient for GPU's given their architecture\n",
        "  LVA = L - LTR\n",
        "  LVA = LVA // B * B # THe same for LVA\n",
        "  BATCH = (BATCH // B + 1) * B # And the same for Batch Size\n",
        "  print(f\"New batch size is: {BATCH}, LVA and LTR are: {LVA}, {LTR} and L is {L}, ... does L>=LTR+LVA? {L>=(LTR+LVA)}\")\n",
        "  return BATCH, L, LVA, LTR\n",
        "\n",
        "\n",
        "def get_refs_for(data):\n",
        "  global T\n",
        "  global SAMPLES\n",
        "  y = np.asarray(data['Y'])\n",
        "  ixs = np.asarray(range(len(y)))\n",
        "  pos = ixs[y==1]\n",
        "  neg = ixs[y==0]\n",
        "\n",
        "  minL = min([len(pos), len(neg)])\n",
        "\n",
        "  # minL/(T+SAMPLES)=SLICES\n",
        "  WIDTH = (T+SAMPLES)\n",
        "  SLICES = minL//WIDTH\n",
        "  assert(SLICES>0)\n",
        "\n",
        "  # Produce the slices\n",
        "  posSlices = [pos[i*WIDTH : (i+1)*WIDTH] for i in range(SLICES)]\n",
        "  negSlices = [neg[i*WIDTH : (i+1)*WIDTH] for i in range(SLICES)]\n",
        "\n",
        "  # Filter only connected slices :-)\n",
        "  TARGET = [0] * (T+SAMPLES-1)\n",
        "  posSlices = [x for x in posSlices if (np.diff(x)-1).tolist() == TARGET]\n",
        "  negSlices = [x for x in negSlices if (np.diff(x)-1).tolist() == TARGET]\n",
        "\n",
        "  # Enforce a tolerance of e.g. 3% difference in length, i.e. how unbalanced the dataset will be.\n",
        "  TOL = max([int(0.03*len(posSlices)),1])\n",
        "  assert(abs(len(posSlices) - len(negSlices)) <= TOL)\n",
        "\n",
        "  # Produce more effective indexes\n",
        "  posRefs = []\n",
        "  for x in posSlices:\n",
        "    posRefs += [x[i+T] for i in range(SAMPLES)]\n",
        "  negRefs = []\n",
        "  for x in negSlices:\n",
        "    negRefs += [x[i+T] for i in range(SAMPLES)]\n",
        "\n",
        "  # Shuffle and return :-)\n",
        "  np.random.shuffle(negRefs)\n",
        "  np.random.shuffle(posRefs)\n",
        "  return negRefs, posRefs\n",
        "\n",
        "\n",
        "def build_data(datas,n):\n",
        "  global MAXPAD\n",
        "  # Open data\n",
        "  data = datas[n]\n",
        "  X1, X20, Y = data['X1'],data['X2'],data['Y']\n",
        "\n",
        "  # Process Y\n",
        "  ONE_HOT_Y = np.zeros((len(Y),2))\n",
        "  for i in range(len(Y)):\n",
        "      ONE_HOT_Y[i,Y[i]] = 1\n",
        "  ONE_HOT_Y = ONE_HOT_Y.astype('float32')\n",
        "\n",
        "  # Process X1\n",
        "  X1_TRACKER = []\n",
        "  for i_,X_ in enumerate(X1):\n",
        "      X1_TRACKER.append((MAXPAD-len(X_)))\n",
        "      X1[i_] = [float(x) / UNIQUES for x in X_]\n",
        "\n",
        "  # Process X2\n",
        "  X2 = [[] for _ in range(len(X20))]\n",
        "  X2_TRACKER = []\n",
        "  for i,X_ in enumerate(X20):\n",
        "      c = 0\n",
        "      for i_,y_ in enumerate(X_):\n",
        "          for z in y_:\n",
        "              X2[i] += [[float(i_)/MAXPAD,float(z)/MAXPAD]]\n",
        "              c += 1\n",
        "      X2_TRACKER.append(MAXPAD-c) \n",
        "\n",
        "  # Record the results\n",
        "  datas[n]['X1'] = (X1, X1_TRACKER)\n",
        "  datas[n]['X2'] = (X2, X2_TRACKER)\n",
        "  datas[n]['Y'] = (ONE_HOT_Y,)\n",
        "\n",
        "  return\n",
        "\n",
        "         \n",
        "def produce_data(A,B,Refs):\n",
        "    counter = 0  \n",
        "    w = list(range(A,B))\n",
        "    np.random.shuffle(w)\n",
        "    YS = datas[PROCS[0]]['Y'][0]\n",
        "    LIMIT  = 2 * (B-A)\n",
        "    while counter < LIMIT:\n",
        "            i = Refs[counter % 2][A + counter // 2]\n",
        "            yield (\n",
        "                    tuple([\n",
        "                  ( tf.convert_to_tensor(np.asarray([ (datas[nproc]['X1'][0][j] + [0.] * datas[nproc]['X1'][1][j]) for j in range(i-T+1,i+1)]).reshape(1,T,-1,1)), \n",
        "                tf.convert_to_tensor(np.asarray([ (datas[nproc]['X2'][0][j] + [[0., 0.]] * datas[nproc]['X2'][1][j]) for j in range(i-T+1,i+1)]).reshape(1,T,-1,2)),\n",
        "              ) for nproc in range(len(PROCS))\n",
        "                        ]), \n",
        "                    tf.convert_to_tensor(YS[i:i+1,:].reshape(1,2)),\n",
        "                  )\n",
        "            counter += 1\n",
        "\n",
        "if not BYPASS_PREVIOUS_SECTION:\n",
        "  trimmer(datas)\n",
        "  Refs  = get_refs_for(datas[1])\n",
        "  BATCH, L, LVA, LTR = recomputer(len(Refs[0]))\n",
        "  print(f'Len of refs is {len(Refs[0])}, L,LVA,LTR are {L},{LVA},{LTR}')\n",
        "  for n in PROCS:\n",
        "    build_data(datas,n)\n",
        "\n",
        "\n",
        "def produce_entire_datasets(L0,Lref, name=''):\n",
        "  \"\"\"\n",
        "  This function attempts to fit the entire dataset into memory.\n",
        "  For a 30-secs kernel trace, approx 10 Mb of pickle data per processor, it fails.\n",
        "  The key is that it includes padding ;-)\n",
        "  \"\"\"\n",
        "  global Refs\n",
        "  it = produce_data(L0,Lref, Refs)\n",
        "  data = []\n",
        "  counter = 0\n",
        "  while True:\n",
        "    try:\n",
        "      data += [it.__next__()]\n",
        "    except:\n",
        "      break\n",
        "    counter += 1\n",
        "  return data\n",
        "\n",
        "\n",
        "def _bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "    if isinstance(value, type(tf.constant(0))): # if value ist tensor\n",
        "        value = value.numpy() # get value of tensor\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "FEATURE_NAME = []\n",
        "for i in range(len(PROCS)):\n",
        "  FEATURE_NAME += [f\"X0_P{i}\", f\"X1_P{i}\"]\n",
        "FEATURE_NAME += [\"Y\"]\n",
        "parse_dic = {\n",
        "    fname: tf.io.FixedLenFeature([], tf.string) for fname in FEATURE_NAME\n",
        "    }\n",
        "def _parse_tfr_element(element):\n",
        "  example_message = tf.io.parse_single_example(element, parse_dic)\n",
        "  byteFeatures = [example_message.get(FEATURE_NAME[i],[]) for i in range(BASE)]\n",
        "  features = [tf.io.parse_tensor(x, out_type=tf.float32) for x in byteFeatures]\n",
        "  return features    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ykTxt9Ll-8Y",
        "outputId": "65eb5075-39f2-4457-cd94-6e0c99e3ded9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old batch size was: 128\n",
            "New batch size is: 160, LVA and LTR are: 448, 1344 and L is 1800, ... does L>=LTR+LVA? True\n",
            "Len of refs is 1800, L,LVA,LTR are 1800,448,1344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<b>Training the Neural Net</b>"
      ],
      "metadata": {
        "id": "oteqJUB2ZR54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_ITERATOR_APPROACH = [True,False][1]\n",
        "if BYPASS_PREVIOUS_SECTION:\n",
        "  # Bypass overwrites the iterator approach and forces the TFRecord pipeline\n",
        "  DATASET_ITERATOR_APPROACH = False"
      ],
      "metadata": {
        "id": "RPQo6KISZ6-1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>A) if the dataset iterator GIL-bound approach is used</b>"
      ],
      "metadata": {
        "id": "IYOMEv-lGDM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if DATASET_ITERATOR_APPROACH:\n",
        "  OS = (\n",
        "          tuple([\n",
        "            (tf.TensorSpec(shape=(None,T,MAXPAD,1), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None,T,MAXPAD,2), dtype=tf.float32)\n",
        "            ) for nproc in range(len(PROCS)) \n",
        "            ]),\n",
        "            tf.TensorSpec(shape=(None,NCATEGORIES), dtype=tf.float32),\n",
        "      )\n",
        "  print(f'Finished building the output structure')\n",
        "  trainD = tf.data.Dataset.from_generator(lambda: produce_data(0,LTR, Refs), output_signature=OS)#output_types=(tf.float32), output_shapes=OS)\n",
        "  print(f'Finished building the training dataset')\n",
        "  valD = tf.data.Dataset.from_generator(lambda: produce_data(LTR,L, Refs), output_signature=OS)# output_types=(tf.float32), output_shapes=OS)\n",
        "  print(f'Finished building the validation dataset')\n",
        "  print(f'About to train! :-)')\n",
        "  history = model.fit(trainD, epochs=10, batch_size=BATCH, validation_data=valD, verbose=2)\n",
        "  print('Finished training! :-)')"
      ],
      "metadata": {
        "id": "LPBNnZ9HRXHB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>B) If the TFRecord IO-bound approach is used</b>"
      ],
      "metadata": {
        "id": "nlT77CLqGL-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not DATASET_ITERATOR_APPROACH and not BYPASS_PREVIOUS_SECTION:\n",
        "\n",
        "  # Define effective-Lengths (eL). Recall that for each data we have a positive(+) and negative(-) sample\n",
        "  eLTR = 2*LTR\n",
        "  eL = 2*L \n",
        "\n",
        "  # Spawn data iterators\n",
        "  trainD = produce_data(0,LTR, Refs)\n",
        "  valD = produce_data(LTR,L, Refs)\n",
        "    \n",
        "  # Prepare variables, configuration, containers, and write the entire\n",
        "  # dataset to a file that can be progressively piped later. \n",
        "  # The point is that TensorFlow can prefetch and/or use parallelization\n",
        "  # to loop more efficiently, as opposed to the GIL impacting on the \n",
        "  # performance of the dataset from generator in the typical training :-)\n",
        "  NFILESTRAIN = 8\n",
        "  NFILESVAL = 4\n",
        "  train_file_paths = [f'data.tfrecords-train{i}' for i in range(NFILESTRAIN)]\n",
        "  THR_TRAINF = eLTR//NFILESTRAIN\n",
        "  val_file_paths = [f'data.tfrecords-val{i}' for i in range(NFILESVAL)]\n",
        "  THR_VALF = (eL-eLTR)//NFILESVAL\n",
        "  BASE = 2 * len(PROCS) + 1\n",
        "  TEST = [True, False][1]\n",
        "  KEEP_GOING = True\n",
        "  counter = 0\n",
        "  data = []\n",
        "  nameix = 0\n",
        "  while KEEP_GOING:\n",
        "    try:\n",
        "      if TEST:\n",
        "        if counter==eLTR-1: raise Exception(\"Test\")\n",
        "      else:\n",
        "        data += [trainD.__next__()]\n",
        "    except:\n",
        "      KEEP_GOING = False\n",
        "    if (counter+1)%THR_TRAINF == 0:# or not KEEP_GOING:\n",
        "      if TEST:\n",
        "        print('at counter: ',counter, ' nameix: ',nameix, ' L: ', eLTR)\n",
        "      else:\n",
        "        arrays = []\n",
        "        for dat in data:\n",
        "          local_array = []\n",
        "          for P in dat[0]:\n",
        "            local_array += [tf.io.serialize_tensor(P[0].astype('float32')), tf.io.serialize_tensor(P[1].astype('float32'))]\n",
        "          local_array += [tf.io.serialize_tensor(dat[1])]\n",
        "          arrays.append(local_array.copy())\n",
        "        with tf.io.TFRecordWriter(train_file_paths[nameix]) as writer:\n",
        "          for serialized_arrays in arrays:\n",
        "            features = {FEATURE_NAME[c]: _bytes_feature(serialized_array) for c,serialized_array in enumerate(serialized_arrays)}\n",
        "            example_message = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "            writer.write(example_message.SerializeToString())\n",
        "        del data\n",
        "        data = []\n",
        "      nameix += 1\n",
        "    counter += 1\n",
        "\n",
        "  KEEP_GOING = True  \n",
        "  counter = 0\n",
        "  data = []\n",
        "  nameix = 0\n",
        "  while KEEP_GOING:\n",
        "    try:\n",
        "      if TEST:\n",
        "        if counter==(eL-eLTR-1): \n",
        "          raise Exception(\"Test\")\n",
        "      else:\n",
        "        data += [valD.__next__()]\n",
        "    except:\n",
        "      KEEP_GOING = False\n",
        "    if (counter+1)%THR_VALF == 0:# or not KEEP_GOING:\n",
        "      if TEST:\n",
        "        print('at counter: ',counter, ' nameix: ',nameix, ' L: ', eL-eLTR)\n",
        "      else:\n",
        "        arrays = []\n",
        "        for dat in data:\n",
        "          local_array = []\n",
        "          for P in dat[0]:\n",
        "            local_array += [tf.io.serialize_tensor(P[0].astype('float32')), tf.io.serialize_tensor(P[1].astype('float32'))]\n",
        "          local_array += [tf.io.serialize_tensor(dat[1])]\n",
        "          arrays.append(local_array.copy())\n",
        "        with tf.io.TFRecordWriter(val_file_paths[nameix]) as writer:\n",
        "          for serialized_arrays in arrays:\n",
        "            features = {FEATURE_NAME[c]: _bytes_feature(serialized_array) for c,serialized_array in enumerate(serialized_arrays)}\n",
        "            example_message = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "            writer.write(example_message.SerializeToString())\n",
        "        del data\n",
        "        data = []\n",
        "      nameix += 1\n",
        "    counter += 1\n",
        "  del(arrays)"
      ],
      "metadata": {
        "id": "Jdr7WzJmZ37f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#  OPTIONAL: store the dataset in your GoogleDrive account, so that in the future you don't have to rebuild it.\n",
        "#  Also: it's not fair to use the Google GPU runtime for the creation of the dataset :-) \n",
        "#\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!mkdir drive/MyDrive/TraceMeOut\n",
        "! rm drive/MyDrive/TraceMeOut/*\n",
        "for nm in train_file_paths + val_file_paths:\n",
        "  os.system(f'cp {nm} drive/MyDrive/TraceMeOut/{nm}')"
      ],
      "metadata": {
        "id": "UJ4D33eHXfio"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<b>You can skip to this section and save the preprocessing overhead</b> \n",
        "\n",
        "<i>(IFF (1) it's not the first run, (2) you have saved the preprocessed dataset in Drive and (3) you commit to the TFRecord pipeline)</i>"
      ],
      "metadata": {
        "id": "pyXIuMyBZbYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if BYPASS_PREVIOUS_SECTION or not DATASET_ITERATOR_APPROACH:\n",
        "\n",
        "  print(f'About to train! :-)') \n",
        "  if BYPASS_PREVIOUS_SECTION:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    BASEDIR = 'drive/MyDrive/TraceMeOut/'\n",
        "    train_file_paths = [BASEDIR + x for x in os.listdir(BASEDIR) if 'train' in x]\n",
        "    val_file_paths = [BASEDIR + x for x in os.listdir(BASEDIR) if 'val' in x]\n",
        "    print('correctly gathered the GoogleDrive data :-)')\n",
        "\n",
        "  tfr_dataset = tf.data.TFRecordDataset(train_file_paths, num_parallel_reads=4) \n",
        "  dataset = tfr_dataset.map(_parse_tfr_element)\n",
        "  tfr_val_dataset = tf.data.TFRecordDataset(val_file_paths, num_parallel_reads=4) \n",
        "  val_dataset = tfr_val_dataset.map(_parse_tfr_element)\n",
        "  METRICS = {'val_categorical_accuracy':[],'categorical_accuracy':[],'val_loss':[],'loss':[]}\n",
        "\n",
        "  Bzero = 32 # INITIAL_BATCH: we won't enforce a static batch on IO-bound Ops \n",
        "  UPPER_BATCH_LIMIT_val = (L-LTR)//10 # Max batch size for our 'adaptive algorithm' (Training set)\n",
        "  UPPER_BATCH_LIMIT = LTR//10 # Max batch size for our 'adaptive algorithm' (Validation set)\n",
        "  PROBA_INCREASE = 0.85\n",
        "  BPATH = []\n",
        "  BPATH2 = []\n",
        "  yplus_train = []\n",
        "  yplus_val = []\n",
        "  BPATH_val = []\n",
        "  times = [np.inf]\n",
        "  times2 = [np.inf]\n",
        "  times_val = [np.inf]\n",
        "  characteristic_time = []\n",
        "  characteristic_time2 = []\n",
        "  characteristic_time_val = []\n",
        "  print('about to start the loop...')\n",
        "  for Ep in range(EPOCHS):\n",
        "\n",
        "    counter = 0\n",
        "    starting = True\n",
        "    tepoch0 = time.time()\n",
        "    for k in METRICS.keys():\n",
        "      METRICS[k] += [[]]\n",
        "    B = Bzero \n",
        "    for i,x in enumerate(dataset):\n",
        "      a,b,c,d,e,f,g,h,k = x\n",
        "\n",
        "      if starting:\n",
        "        t0 = time.time() # PArt of the adaptive strategy\n",
        "        tnow = time.time()\n",
        "\n",
        "        X0a = a\n",
        "        X0b = b\n",
        "        X1a = c\n",
        "        X1b = d\n",
        "        X2a = e\n",
        "        X2b = f\n",
        "        X3a = g\n",
        "        X3b = h\n",
        "        Y = [k[0]]\n",
        "        starting = False\n",
        "      else:\n",
        "        X0a = tf.concat([X0a,a],0)\n",
        "        X0b = tf.concat([X0b,b],0)\n",
        "        X1a = tf.concat([X1a,c],0)\n",
        "        X1b = tf.concat([X1b,d],0)\n",
        "        X2a = tf.concat([X2a,e],0)\n",
        "        X2b = tf.concat([X2b,f],0)\n",
        "        X3a = tf.concat([X3a,g],0)\n",
        "        X3b = tf.concat([X3b,h],0)\n",
        "        Y.append(k[0])\n",
        "\n",
        "        tnow = time.time()# PArt of the adaptive strategy\n",
        "        # Randomly double the batch with proba 15%\n",
        "        # Also define a criteria for optimizing the batch size: \n",
        "        #                    LinearEstimate(T) > PreviousTime(T): decrease B to the current counter + 1 (and flush)\n",
        "        #         LinearEstimate(T) is  TTakenSoFar + Nsamples * ProcessingTimePerSample\n",
        "        if ((tnow - t0) + (counter+1) * np.mean(characteristic_time)) > times[-1]: \n",
        "          B = counter + 1\n",
        "        if np.random.rand() > PROBA_INCREASE:\n",
        "          if 2*B < UPPER_BATCH_LIMIT: B *= 2\n",
        "      BPATH += [B].copy() # For the record\n",
        "\n",
        "      if (counter + 1) % B == 0:\n",
        "        tbefore = time.time()\n",
        "        with tf.GradientTape() as tape:\n",
        "          logits = model([X0a,X0b,X1a,X1b,X2a,X2b,X3a,X3b], training=True) \n",
        "          loss_value = model.loss(Y, logits)\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        model.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        #METRICS['loss'][Ep] += [(loss_value.numpy(), len(Y))] \n",
        "        #METRICS['categorical_accuracy'][Ep] += [(tf.keras.metrics.categorical_accuracy(Y,logits).numpy().mean(), len(Y))]\n",
        "        yplus_train += [(np.sum(np.asarray(Y)[:,1]),len(Y))]\n",
        "        tafter = time.time()\n",
        "        characteristic_time.append((tafter-tbefore)/len(Y))\n",
        "        times.append((tafter - t0))\n",
        "        starting = True\n",
        "        counter = 0\n",
        "      counter += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    starting = True\n",
        "    B = Bzero \n",
        "    counter = 0\n",
        "    for i,x in enumerate(dataset):\n",
        "      a,b,c,d,e,f,g,h,k = x\n",
        "\n",
        "      if starting:\n",
        "        t0 = time.time()\n",
        "        tnow = time.time()\n",
        "\n",
        "        X0a = a\n",
        "        X0b = b\n",
        "        X1a = c\n",
        "        X1b = d\n",
        "        X2a = e\n",
        "        X2b = f\n",
        "        X3a = g\n",
        "        X3b = h\n",
        "        Y = [k[0]]\n",
        "        starting = False\n",
        "      else:\n",
        "        X0a = tf.concat([X0a,a],0)\n",
        "        X0b = tf.concat([X0b,b],0)\n",
        "        X1a = tf.concat([X1a,c],0)\n",
        "        X1b = tf.concat([X1b,d],0)\n",
        "        X2a = tf.concat([X2a,e],0)\n",
        "        X2b = tf.concat([X2b,f],0)\n",
        "        X3a = tf.concat([X3a,g],0)\n",
        "        X3b = tf.concat([X3b,h],0)\n",
        "        Y.append(k[0])\n",
        "\n",
        "        tnow = time.time()# PArt of the adaptive strategy\n",
        "        # Randomly double the batch with proba 15%\n",
        "        # Also define a criteria for optimizing the batch size: \n",
        "        #                    LinearEstimate(T) > PreviousTime(T): decrease B to the current counter + 1 (and flush)\n",
        "        #         LinearEstimate(T) is  TTakenSoFar + Nsamples * ProcessingTimePerSample\n",
        "        if ((tnow - t0) + (counter+1) * np.mean(characteristic_time2)) > times2[-1]: \n",
        "          B = counter + 1\n",
        "        if np.random.rand() > PROBA_INCREASE:\n",
        "          if 2*B < UPPER_BATCH_LIMIT: B *= 2\n",
        "      BPATH2 += [B].copy() # For the record\n",
        "\n",
        "      if (counter + 1) % B == 0:\n",
        "        tbefore = time.time()\n",
        "        logits = model([X0a,X0b,X1a,X1b,X2a,X2b,X3a,X3b], training=False) \n",
        "        METRICS['loss'][Ep] += [(model.loss(Y, logits).numpy(), len(Y))] \n",
        "        METRICS['categorical_accuracy'][Ep] += [(tf.keras.metrics.categorical_accuracy(Y,logits).numpy().mean(), len(Y))]\n",
        "        tafter = time.time()\n",
        "        characteristic_time2.append((tafter-tbefore)/len(Y))\n",
        "        times2.append((tafter - t0))\n",
        "        starting = True\n",
        "        counter = 0\n",
        "      counter += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    starting = True\n",
        "    counter = 0\n",
        "    B = Bzero \n",
        "    for i,x in enumerate(val_dataset):\n",
        "      a,b,c,d,e,f,g,h,k = x\n",
        "      # We can still add a layer of control by saving the data with some batches,e.g. groups of 2 or 4 :-) POTENTIAL UPGRADE\n",
        "      if starting:\n",
        "        t0 = time.time()\n",
        "        tnow = time.time()\n",
        "\n",
        "        X0a = a\n",
        "        X0b = b\n",
        "        X1a = c\n",
        "        X1b = d\n",
        "        X2a = e\n",
        "        X2b = f\n",
        "        X3a = g\n",
        "        X3b = h\n",
        "        Y = [k[0]]\n",
        "        starting = False\n",
        "      else:\n",
        "        X0a = tf.concat([X0a,a],0)\n",
        "        X0b = tf.concat([X0b,b],0)\n",
        "        X1a = tf.concat([X1a,c],0)\n",
        "        X1b = tf.concat([X1b,d],0)\n",
        "        X2a = tf.concat([X2a,e],0)\n",
        "        X2b = tf.concat([X2b,f],0)\n",
        "        X3a = tf.concat([X3a,g],0)\n",
        "        X3b = tf.concat([X3b,h],0)\n",
        "        Y.append(k[0])\n",
        "\n",
        "        tnow = time.time()# PArt of the adaptive strategy\n",
        "        # Randomly double the batch with proba 15%\n",
        "        # Also define a criteria for optimizing the batch size: \n",
        "        #                    LinearEstimate(T) > PreviousTime(T): decrease B to the current counter + 1 (and flush)\n",
        "        #         LinearEstimate(T) is  TTakenSoFar + Nsamples * ProcessingTimePerSample\n",
        "        if ((tnow - t0) + (counter+1) * np.mean(characteristic_time_val)) > times_val[-1]: \n",
        "          B = counter + 1\n",
        "        if np.random.rand() > PROBA_INCREASE:\n",
        "          if 2*B < UPPER_BATCH_LIMIT_val: B *= 2\n",
        "      BPATH_val += [B].copy() # For the record\n",
        "\n",
        "      if (counter + 1) % B == 0:\n",
        "        tbefore = time.time()\n",
        "        logits = model([X0a,X0b,X1a,X1b,X2a,X2b,X3a,X3b], training=False) \n",
        "        METRICS['val_loss'][Ep] += [(model.loss(Y, logits).numpy(), len(Y))] \n",
        "        METRICS['val_categorical_accuracy'][Ep] += [(tf.keras.metrics.categorical_accuracy(Y,logits).numpy().mean(), len(Y))]\n",
        "        tafter = time.time()\n",
        "        characteristic_time_val.append((tafter-tbefore)/len(Y))\n",
        "        times_val.append((tafter - t0))\n",
        "        starting = True\n",
        "        counter = 0\n",
        "        yplus_val += [(np.sum(np.asarray(Y)[:,1]),len(Y))]        \n",
        "      counter += 1\n",
        "\n",
        "    # Time report\n",
        "    tfinal = time.time()\n",
        "    print(f'Time for Epoch {Ep} and batch size {B} has been {tfinal-tepoch0} seconds')\n",
        "\n",
        "  print('Finished training! :-)')  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5keq7euK71c-",
        "outputId": "8f984c28-e876-4834-d2fb-52291de5eb16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "About to train! :-)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "correctly gathered the GoogleDrive data :-)\n",
            "about to start the loop...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for Epoch 0 and batch size 10 has been 178.31106972694397 seconds\n",
            "Time for Epoch 1 and batch size 3 has been 198.1235933303833 seconds\n",
            "Time for Epoch 2 and batch size 2 has been 220.0417184829712 seconds\n",
            "Time for Epoch 3 and batch size 2 has been 219.79900670051575 seconds\n",
            "Time for Epoch 4 and batch size 2 has been 201.36557865142822 seconds\n",
            "Time for Epoch 5 and batch size 2 has been 193.50370836257935 seconds\n",
            "Time for Epoch 6 and batch size 2 has been 219.92980670928955 seconds\n",
            "Time for Epoch 7 and batch size 2 has been 228.74314546585083 seconds\n",
            "Time for Epoch 8 and batch size 2 has been 228.0845398902893 seconds\n",
            "Time for Epoch 9 and batch size 2 has been 228.71560096740723 seconds\n",
            "Time for Epoch 10 and batch size 2 has been 228.50530195236206 seconds\n",
            "Time for Epoch 11 and batch size 2 has been 193.63364553451538 seconds\n",
            "Time for Epoch 12 and batch size 2 has been 219.9080846309662 seconds\n",
            "Time for Epoch 13 and batch size 2 has been 219.43738675117493 seconds\n",
            "Time for Epoch 14 and batch size 2 has been 220.2053701877594 seconds\n",
            "Time for Epoch 15 and batch size 2 has been 202.63335371017456 seconds\n",
            "Time for Epoch 16 and batch size 2 has been 202.77623653411865 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if BYPASS_PREVIOUS_SECTION or not DATASET_ITERATOR_APPROACH:\n",
        "  # Plot the batch size over time :-)\n",
        "  plt.plot(BPATH)\n",
        "  plt.title('Adaptive Batch Size over time (training)')\n",
        "  plt.show()\n",
        "  plt.plot(BPATH2)\n",
        "  plt.title('Adaptive Batch Size over time (evaluation over training set)')\n",
        "  plt.show()\n",
        "  plt.plot(BPATH_val)\n",
        "  plt.title('Adaptive Batch Size over time (evaluation over validation set)')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "azGM9eESE68I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the metrics into the correct format \n",
        "if BYPASS_PREVIOUS_SECTION or not DATASET_ITERATOR_APPROACH:\n",
        "\n",
        "  # How unbalanced was the database? For the training dataset\n",
        "  train_unbalance = 0\n",
        "  S,T = 0,0\n",
        "  for yp in yplus_train:\n",
        "    T += yp[1]\n",
        "    S += yp[0]\n",
        "  train_unbalance = S / T\n",
        "\n",
        "\n",
        "  # How unbalanced was the database? For the validation dataset\n",
        "  val_unbalance = 0\n",
        "  S,T = 0,0\n",
        "  for yp in yplus_val:\n",
        "    T += yp[1]\n",
        "    S += yp[0]\n",
        "  val_unbalance = S / T\n",
        "\n",
        "  print(f'The training fraction of positives was: {train_unbalance}')\n",
        "  print(f'The vallidation fraction of positives was: {val_unbalance}')\n",
        "    \n",
        "  # Postprocess TFRecord data :-)\n",
        "  ks = list(METRICS.keys())\n",
        "  PROCESSED_METRICS = {}\n",
        "  for k in ks:\n",
        "    PROCESSED_METRICS[k] = []\n",
        "    for y in METRICS[k]:\n",
        "      S = sum(x[1] for x in y)\n",
        "      R = sum([x[0] * x[1] / S for x in y])\n",
        "      PROCESSED_METRICS[k].append(R)"
      ],
      "metadata": {
        "id": "DbCi0MR9Rbsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<b>Plotting the Neural Net results</b>"
      ],
      "metadata": {
        "id": "ildO_9EYZVol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results\n",
        "if not BYPASS_PREVIOUS_SECTION and DATASET_ITERATOR_APPROACH:\n",
        "  results = history.history\n",
        "else:\n",
        "  results = PROCESSED_METRICS\n",
        "\n",
        "f,ax = plt.subplots(1,2,figsize=(15,10))\n",
        "\n",
        "ax[0].plot(results['loss'],label='loss', c='k', lw=2)\n",
        "ax[0].plot(results['val_loss'], label='val loss', c='r', lw=2)\n",
        "ax[0].grid()\n",
        "ax[0].set_title('Training and Validation Loss* over the Epochs\\n*TensorFlow categorical crossentropy')\n",
        "std = max([abs(max(results['loss'])-min(results['val_loss'])), abs(max(results['val_loss'])-min(results['loss']))])\n",
        "ax[0].set_ylim(min([min(results['loss']), min(results['val_loss'])])-std,\n",
        "               max([max(results['loss']), max(results['val_loss'])])+std)\n",
        "ax[0].set_yscale('log')\n",
        "ax[0].set_xticks(range(EPOCHS))\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(results['categorical_accuracy'],label='accuracy', c='k', lw=2)\n",
        "ax[1].plot(results['val_categorical_accuracy'], label='val accuracy', c='r', lw=2)\n",
        "ax[1].set_ylim(0.45,1)\n",
        "if DATASET_ITERATOR_APPROACH:\n",
        "  ax[1].hlines(0.5, 0, EPOCHS, color='y', ls=':', lw=4, label='Null Hypothesis\\n(perfect 50% tag balance case)')\n",
        "else:\n",
        "  ax[1].hlines(val_unbalance, 0, EPOCHS, color='y', ls='--', lw=4, label='Null Hypothesis (validation)')\n",
        "  ax[1].hlines(train_unbalance, 0, EPOCHS, color='g', ls=':', lw=4, label='Null Hypothesis (training)')\n",
        "ax[1].grid()\n",
        "ax[1].set_title('Training and Validation Acurracy over the Epochs')\n",
        "ax[1].legend()\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rktl-jpbmLRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the raw values of the metrics, just in case we want to use them to make nicer plots \n",
        "# without re-running the notebook :O\n",
        "print(results['loss'])\n",
        "print(results['val_loss'])\n",
        "print(results['categorical_accuracy'])\n",
        "print(results['val_categorical_accuracy'])"
      ],
      "metadata": {
        "id": "j0VBVFx0nMh2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}